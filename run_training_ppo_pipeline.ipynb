{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Annalia321/FastGPT/blob/main/run_training_ppo_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gCUwyevcAIbo"
      },
      "source": [
        "# Training Pipeline\n",
        "[run_training_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_pipeline.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "uPkkiUgMAIbr"
      },
      "source": [
        "# Stage 1: Continue Pretraining\n",
        "\n",
        "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
        "\n",
        "注意：\n",
        "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
        "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
        "\n",
        "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KagKbMn0AIbu"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B\n",
        "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyS69_1SAIbv"
      },
      "source": [
        "## 配置运行环境\n",
        "\n",
        "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
        "\n",
        "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
        "\n",
        "步骤：\n",
        "1. 下载最新代码到本地\n",
        "2. 安装依赖包\n",
        "\n",
        "依赖包如下，保证最新版本：\n",
        "\n",
        "```\n",
        "loguru\n",
        "transformers\n",
        "sentencepiece\n",
        "datasets\n",
        "tensorboard\n",
        "tqdm\n",
        "peft\n",
        "trl\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "n9hIgttJAIbw",
        "outputId": "0d638874-e889-4609-a20b-406adbffed1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedicalGPT'...\n",
            "remote: Enumerating objects: 92, done.\u001b[K\n",
            "remote: Counting objects: 100% (92/92), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 92 (delta 17), reused 42 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (92/92), 8.55 MiB | 14.96 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n",
            "/content/MedicalGPT\n",
            "build_domain_tokenizer.py   merge_peft_adapter.py  run_ppo.sh\n",
            "chatpdf.py                  merge_tokenizers.py    run_pt.sh\n",
            "CITATION.cff                model_quant.py         run_quant.sh\n",
            "_config.yml                 openai_api.py          run_rm.sh\n",
            "CONTRIBUTING.md             orpo_training.py       run_sft.sh\n",
            "convert_dataset.py          ppo_training.py        run_training_dpo_pipeline.ipynb\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/                       pretraining.py         run_training_ppo_pipeline.ipynb\n",
            "DISCLAIMER                  README_EN.md           supervised_finetuning.py\n",
            "\u001b[01;34mdocs\u001b[0m/                       README.md              template.py\n",
            "dpo_training.py             requirements.txt       validate_jsonl.py\n",
            "eval_quantize.py            reward_modeling.py     vllm_deployment.sh\n",
            "fastapi_server_demo.py      \u001b[01;34mrole_play_data\u001b[0m/        zero1.yaml\n",
            "gradio_demo.py              run_dpo.sh             zero2.json\n",
            "grpo_training.py            run_eval_quantize.sh   zero2.yaml\n",
            "inference_multigpu_demo.py  run_full_sft.sh        zero3.json\n",
            "inference.py                run_grpo.sh            zero3.yaml\n",
            "LICENSE                     run_orpo.sh\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting datasets>=2.14.6 (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting loguru (from -r requirements.txt (line 3))\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: peft>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.14.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.18.0)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.67.1)\n",
            "Collecting transformers>=4.49.0 (from -r requirements.txt (line 9))\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.15.2 (from -r requirements.txt (line 10))\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.10.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting math-verify==0.5.2 (from -r requirements.txt (line 13))\n",
            "  Downloading math_verify-0.5.2-py3-none-any.whl.metadata (347 bytes)\n",
            "Collecting latex2sympy2_extended (from -r requirements.txt (line 12))\n",
            "  Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.13.2 (from latex2sympy2_extended->-r requirements.txt (line 12))\n",
            "  Downloading antlr4_python3_runtime-4.13.2-py3-none-any.whl.metadata (304 bytes)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from latex2sympy2_extended->-r requirements.txt (line 12)) (1.13.1)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.32.3)\n",
            "Collecting xxhash (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.14.6->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.11.13)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->-r requirements.txt (line 7)) (3.1.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.49.0->-r requirements.txt (line 9)) (0.21.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl>=0.15.2->-r requirements.txt (line 10)) (13.9.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate->-r requirements.txt (line 1)) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->latex2sympy2_extended->-r requirements.txt (line 12)) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl>=0.15.2->-r requirements.txt (line 10)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl>=0.15.2->-r requirements.txt (line 10)) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl>=0.15.2->-r requirements.txt (line 10)) (0.1.2)\n",
            "Downloading math_verify-0.5.2-py3-none-any.whl (27 kB)\n",
            "Downloading latex2sympy2_extended-1.0.6-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading antlr4_python3_runtime-4.13.2-py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: antlr4-python3-runtime, xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, loguru, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, latex2sympy2_extended, nvidia-cusolver-cu12, math-verify, transformers, datasets, trl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed antlr4-python3-runtime-4.13.2 datasets-3.4.1 dill-0.3.8 latex2sympy2_extended-1.0.6 loguru-0.7.3 math-verify-0.5.2 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 transformers-4.49.0 trl-0.15.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
        "%cd MedicalGPT\n",
        "%ls\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SHicioKAIby"
      },
      "source": [
        "## Stage1 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyOwhMQbAIbz"
      },
      "outputs": [],
      "source": [
        "%ls ./data/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "U2kKJQKbAIbz",
        "outputId": "971fc148-77e1-4511-e0cf-2f851b67e722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-18 16:30:59.236561: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742315459.471440    6474 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742315459.532665    6474 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-18 16:31:00.029386: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-03-18 16:31:06.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='deepseek-ai/deepseek-llm-7b-base', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:06.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/pretrain', validation_file_dir='./data/pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:06.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m361\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=True,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.0002,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-pt-v1/runs/Mar18_16-31-05_5e53693f8c97,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-pt-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=3,\n",
            "per_device_train_batch_size=3,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-pt-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.01,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:06.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m362\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:06.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m363\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "tokenizer_config.json: 100% 792/792 [00:00<00:00, 4.69MB/s]\n",
            "tokenizer.json: 100% 4.61M/4.61M [00:00<00:00, 5.15MB/s]\n",
            "\u001b[32m2025-03-18 16:31:10.871\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m471\u001b[0m - \u001b[1mtrain files: ['./data/pretrain/tianlongbabu.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/fever.txt']\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:10.880\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m481\u001b[0m - \u001b[1meval files: ['./data/pretrain/tianlongbabu.txt', './data/pretrain/en_article_tail500.txt', './data/pretrain/fever.txt']\u001b[0m\n",
            "Generating train split: 3876 examples [00:00, 140777.46 examples/s]\n",
            "Generating validation split: 3876 examples [00:00, 401301.43 examples/s]\n",
            "\u001b[32m2025-03-18 16:31:11.827\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m513\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text'],\n",
            "        num_rows: 3876\n",
            "    })\n",
            "})\u001b[0m\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:01<00:00, 3633.57 examples/s]\n",
            "Running tokenizer on dataset: 100% 3876/3876 [00:00<00:00, 4059.27 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 5101.95 examples/s]\n",
            "Grouping texts in chunks of 128: 100% 3876/3876 [00:00<00:00, 8156.30 examples/s]\n",
            "\u001b[32m2025-03-18 16:31:15.217\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m576\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2646\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:15.217\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m577\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:15.218\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m578\u001b[0m - \u001b[34m\u001b[1m<｜begin▁of▁sentence｜>天龙八部\n",
            "<｜begin▁of▁sentence｜>\n",
            "<｜begin▁of▁sentence｜>\n",
            "<｜begin▁of▁sentence｜>正文 释名\n",
            "<｜begin▁of▁sentence｜>“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "<｜begin▁of▁sentence｜>“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:15.220\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m590\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:15.220\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m591\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-03-18 16:31:15.221\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m592\u001b[0m - \u001b[34m\u001b[1m<｜begin▁of▁sentence｜>天龙八部\n",
            "<｜begin▁of▁sentence｜>\n",
            "<｜begin▁of▁sentence｜>\n",
            "<｜begin▁of▁sentence｜>正文 释名\n",
            "<｜begin▁of▁sentence｜>“天龙八部”这名词出于佛经。许多大乘佛经叙述佛向诸菩萨、比丘等说法时，崐常有天龙八部参与听法。如“法华经：提婆达多品”：“天龙八部、人与非人，皆崐遥见彼龙女成佛”。\n",
            "<｜begin▁of▁sentence｜>“非人”，包括八种神道怪物，因为以“天”及“龙”为首，崐所以称为《天龙八部\u001b[0m\n",
            "config.json: 100% 584/584 [00:00<00:00, 4.11MB/s]\n",
            "pytorch_model.bin.index.json: 100% 22.5k/22.5k [00:00<00:00, 86.4MB/s]\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "pytorch_model-00001-of-00002.bin:   0% 0.00/9.97G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   0% 21.0M/9.97G [00:00<00:52, 188MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 52.4M/9.97G [00:00<00:43, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 83.9M/9.97G [00:00<00:42, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 115M/9.97G [00:00<00:43, 227MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   1% 147M/9.97G [00:00<00:43, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 178M/9.97G [00:00<00:40, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 210M/9.97G [00:00<00:41, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   2% 241M/9.97G [00:01<00:40, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 273M/9.97G [00:01<01:15, 129MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 304M/9.97G [00:01<01:02, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   3% 336M/9.97G [00:01<00:53, 181MB/s]\u001b[A\n",
            "\n",
            "model.safetensors.index.json: 100% 23.6k/23.6k [00:00<00:00, 61.9MB/s]\n",
            "\n",
            "pytorch_model-00001-of-00002.bin:   4% 367M/9.97G [00:01<00:50, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 398M/9.97G [00:02<00:46, 206MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   4% 430M/9.97G [00:02<00:44, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 461M/9.97G [00:02<00:43, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 493M/9.97G [00:02<00:41, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   5% 524M/9.97G [00:02<00:41, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 556M/9.97G [00:02<00:42, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 587M/9.97G [00:02<00:41, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   6% 619M/9.97G [00:02<00:39, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 650M/9.97G [00:03<00:39, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 682M/9.97G [00:03<00:39, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 713M/9.97G [00:03<00:40, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   7% 744M/9.97G [00:03<00:40, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 776M/9.97G [00:03<00:39, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 807M/9.97G [00:03<00:39, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   8% 839M/9.97G [00:03<00:41, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 870M/9.97G [00:04<00:39, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 902M/9.97G [00:04<00:40, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:   9% 933M/9.97G [00:04<00:39, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 965M/9.97G [00:04<00:36, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 996M/9.97G [00:04<00:35, 252MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  10% 1.03G/9.97G [00:04<00:37, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.06G/9.97G [00:04<00:37, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.09G/9.97G [00:04<00:37, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  11% 1.12G/9.97G [00:05<00:38, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.15G/9.97G [00:05<00:36, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.18G/9.97G [00:05<00:34, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  12% 1.22G/9.97G [00:05<00:33, 259MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.25G/9.97G [00:05<00:34, 254MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.28G/9.97G [00:05<00:35, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.31G/9.97G [00:05<00:36, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  13% 1.34G/9.97G [00:05<00:34, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.37G/9.97G [00:06<00:38, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.41G/9.97G [00:06<00:37, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  14% 1.44G/9.97G [00:06<00:36, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.47G/9.97G [00:06<00:36, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.50G/9.97G [00:06<00:36, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  15% 1.53G/9.97G [00:06<00:36, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.56G/9.97G [00:06<00:35, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.59G/9.97G [00:07<00:33, 253MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  16% 1.63G/9.97G [00:07<00:33, 251MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.66G/9.97G [00:07<00:34, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.69G/9.97G [00:07<00:35, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  17% 1.72G/9.97G [00:07<00:35, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.75G/9.97G [00:07<00:36, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.78G/9.97G [00:07<00:35, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  18% 1.81G/9.97G [00:07<00:34, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.85G/9.97G [00:08<00:34, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.88G/9.97G [00:08<00:34, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.91G/9.97G [00:08<00:40, 199MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  19% 1.94G/9.97G [00:08<00:38, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 1.97G/9.97G [00:08<00:36, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.00G/9.97G [00:08<00:35, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  20% 2.03G/9.97G [00:09<00:35, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.07G/9.97G [00:09<00:34, 232MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.10G/9.97G [00:09<00:43, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  21% 2.13G/9.97G [00:09<00:40, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.16G/9.97G [00:09<00:37, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.19G/9.97G [00:09<00:36, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  22% 2.22G/9.97G [00:09<00:35, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.25G/9.97G [00:10<00:37, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.29G/9.97G [00:10<00:40, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.32G/9.97G [00:10<00:38, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  23% 2.34G/9.97G [00:10<00:47, 161MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.38G/9.97G [00:10<00:41, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.40G/9.97G [00:10<00:40, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  24% 2.42G/9.97G [00:11<00:42, 178MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.44G/9.97G [00:11<00:43, 171MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.46G/9.97G [00:11<00:42, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.49G/9.97G [00:11<00:41, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.51G/9.97G [00:11<00:40, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  25% 2.54G/9.97G [00:11<00:38, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.56G/9.97G [00:11<00:39, 188MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.58G/9.97G [00:18<10:52, 11.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  26% 2.62G/9.97G [00:18<06:14, 19.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.65G/9.97G [00:18<04:24, 27.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.68G/9.97G [00:18<03:09, 38.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  27% 2.72G/9.97G [00:18<02:19, 51.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.75G/9.97G [00:18<01:48, 66.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.78G/9.97G [00:19<01:22, 87.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  28% 2.81G/9.97G [00:19<01:05, 109MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.84G/9.97G [00:19<00:54, 130MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.87G/9.97G [00:19<00:46, 151MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.90G/9.97G [00:19<00:40, 176MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  29% 2.94G/9.97G [00:19<00:37, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 2.97G/9.97G [00:19<00:34, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.00G/9.97G [00:19<00:32, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  30% 3.03G/9.97G [00:20<00:31, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.06G/9.97G [00:20<00:31, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.09G/9.97G [00:20<00:37, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  31% 3.12G/9.97G [00:20<00:35, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.16G/9.97G [00:20<00:32, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.19G/9.97G [00:20<00:31, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  32% 3.22G/9.97G [00:21<00:30, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.25G/9.97G [00:21<01:01, 109MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.27G/9.97G [00:21<00:55, 121MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.29G/9.97G [00:22<02:13, 50.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  33% 3.31G/9.97G [00:24<03:40, 30.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.34G/9.97G [00:24<02:30, 44.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.38G/9.97G [00:24<01:48, 60.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  34% 3.41G/9.97G [00:24<01:22, 79.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.44G/9.97G [00:24<01:04, 101MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.47G/9.97G [00:25<00:52, 123MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.50G/9.97G [00:25<00:44, 144MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  35% 3.53G/9.97G [00:25<00:39, 164MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.57G/9.97G [00:25<00:36, 178MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.60G/9.97G [00:25<00:33, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  36% 3.63G/9.97G [00:25<00:31, 201MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.66G/9.97G [00:25<00:30, 205MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.69G/9.97G [00:26<00:30, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  37% 3.72G/9.97G [00:26<00:28, 221MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.75G/9.97G [00:26<00:28, 216MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.79G/9.97G [00:26<00:28, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  38% 3.82G/9.97G [00:27<01:26, 71.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.84G/9.97G [00:27<01:14, 82.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.86G/9.97G [00:28<02:14, 45.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.89G/9.97G [00:28<01:37, 62.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  39% 3.92G/9.97G [00:29<01:12, 83.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.95G/9.97G [00:29<00:56, 106MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 3.98G/9.97G [00:29<00:48, 122MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  40% 4.02G/9.97G [00:29<00:41, 145MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.05G/9.97G [00:29<00:35, 165MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.08G/9.97G [00:29<00:32, 182MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  41% 4.11G/9.97G [00:30<00:41, 141MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.14G/9.97G [00:30<00:36, 159MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.16G/9.97G [00:30<00:35, 166MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.19G/9.97G [00:31<01:33, 62.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.22G/9.97G [00:31<01:17, 74.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  42% 4.24G/9.97G [00:32<02:18, 41.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.26G/9.97G [00:34<03:38, 26.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.29G/9.97G [00:34<02:25, 39.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  43% 4.32G/9.97G [00:34<01:43, 54.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.35G/9.97G [00:34<01:17, 72.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.37G/9.97G [00:34<01:05, 85.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.40G/9.97G [00:35<00:50, 110MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  44% 4.44G/9.97G [00:35<00:41, 133MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.47G/9.97G [00:35<00:35, 154MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.50G/9.97G [00:35<00:34, 158MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  45% 4.53G/9.97G [00:35<00:30, 177MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.56G/9.97G [00:35<00:28, 192MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.59G/9.97G [00:35<00:26, 204MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  46% 4.62G/9.97G [00:36<00:24, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.66G/9.97G [00:36<00:25, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.69G/9.97G [00:36<00:28, 184MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  47% 4.71G/9.97G [00:38<02:34, 34.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.74G/9.97G [00:38<01:49, 47.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.77G/9.97G [00:39<01:21, 63.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.80G/9.97G [00:39<01:02, 82.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  48% 4.83G/9.97G [00:39<00:49, 104MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.87G/9.97G [00:39<00:40, 127MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.90G/9.97G [00:39<00:34, 147MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  49% 4.93G/9.97G [00:39<00:30, 167MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.96G/9.97G [00:39<00:27, 185MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 4.99G/9.97G [00:39<00:25, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  50% 5.02G/9.97G [00:40<00:23, 211MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.05G/9.97G [00:40<00:22, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.09G/9.97G [00:40<00:22, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  51% 5.12G/9.97G [00:40<00:20, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.15G/9.97G [00:40<00:20, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.18G/9.97G [00:40<00:20, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  52% 5.21G/9.97G [00:40<00:24, 193MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.24G/9.97G [00:41<00:23, 205MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.27G/9.97G [00:41<00:24, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.30G/9.97G [00:41<00:23, 197MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  53% 5.32G/9.97G [00:41<00:24, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.34G/9.97G [00:41<00:38, 121MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.36G/9.97G [00:42<00:57, 80.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.38G/9.97G [00:47<05:26, 14.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  54% 5.41G/9.97G [00:47<03:29, 21.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.44G/9.97G [00:47<02:22, 31.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.47G/9.97G [00:47<01:40, 44.7MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  55% 5.51G/9.97G [00:47<01:14, 60.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.54G/9.97G [00:47<00:56, 79.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.57G/9.97G [00:47<00:43, 101MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.60G/9.97G [00:47<00:36, 119MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  56% 5.63G/9.97G [00:48<00:32, 135MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.66G/9.97G [00:48<00:28, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.69G/9.97G [00:48<00:25, 166MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  57% 5.73G/9.97G [00:48<00:23, 181MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.76G/9.97G [00:48<00:22, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.79G/9.97G [00:48<00:21, 191MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  58% 5.82G/9.97G [00:48<00:20, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.85G/9.97G [00:49<00:19, 208MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.88G/9.97G [00:49<00:19, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  59% 5.91G/9.97G [00:49<00:19, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.95G/9.97G [00:49<00:21, 190MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 5.98G/9.97G [00:49<00:27, 146MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.01G/9.97G [00:50<00:23, 167MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  60% 6.03G/9.97G [00:54<03:35, 18.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.05G/9.97G [00:55<03:01, 21.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.07G/9.97G [00:55<02:19, 27.9MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.10G/9.97G [00:55<01:35, 40.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  61% 6.12G/9.97G [00:55<01:15, 50.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.16G/9.97G [00:55<00:54, 70.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.19G/9.97G [00:55<00:41, 91.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  62% 6.22G/9.97G [00:55<00:32, 114MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.25G/9.97G [00:56<00:28, 133MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.28G/9.97G [00:56<00:23, 155MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  63% 6.31G/9.97G [00:56<00:21, 173MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.34G/9.97G [00:56<00:19, 186MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.38G/9.97G [00:56<00:18, 198MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  64% 6.41G/9.97G [00:56<00:17, 207MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.44G/9.97G [00:56<00:16, 217MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.47G/9.97G [00:57<00:16, 212MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  65% 6.50G/9.97G [00:57<00:14, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.53G/9.97G [00:57<00:14, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.56G/9.97G [00:57<00:14, 239MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.60G/9.97G [00:57<00:14, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  66% 6.63G/9.97G [00:57<00:14, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.66G/9.97G [00:57<00:14, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.69G/9.97G [00:57<00:13, 237MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  67% 6.72G/9.97G [00:58<00:13, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.75G/9.97G [00:58<00:13, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.78G/9.97G [00:58<00:13, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  68% 6.82G/9.97G [00:58<00:13, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.85G/9.97G [00:58<00:12, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.88G/9.97G [00:58<00:12, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  69% 6.91G/9.97G [00:58<00:12, 244MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.94G/9.97G [00:59<00:13, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 6.97G/9.97G [00:59<00:12, 248MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  70% 7.00G/9.97G [00:59<00:11, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.04G/9.97G [00:59<00:12, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.07G/9.97G [00:59<00:11, 246MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  71% 7.10G/9.97G [00:59<00:11, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.13G/9.97G [00:59<00:11, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.16G/9.97G [00:59<00:11, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.19G/9.97G [01:00<00:11, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  72% 7.22G/9.97G [01:00<00:11, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.26G/9.97G [01:00<00:11, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.29G/9.97G [01:00<00:11, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  73% 7.32G/9.97G [01:00<00:11, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.35G/9.97G [01:00<00:11, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.38G/9.97G [01:00<00:10, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  74% 7.41G/9.97G [01:01<00:10, 241MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.44G/9.97G [01:01<00:10, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.48G/9.97G [01:01<00:10, 238MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  75% 7.51G/9.97G [01:01<00:10, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.54G/9.97G [01:01<00:10, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.57G/9.97G [01:01<00:10, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.60G/9.97G [01:07<02:18, 17.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  76% 7.62G/9.97G [01:07<01:49, 21.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.64G/9.97G [01:07<01:25, 27.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.68G/9.97G [01:07<00:59, 38.8MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  77% 7.71G/9.97G [01:08<00:42, 53.2MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.74G/9.97G [01:08<00:31, 71.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.77G/9.97G [01:08<00:24, 90.4MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  78% 7.80G/9.97G [01:08<00:19, 113MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.83G/9.97G [01:08<00:16, 132MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.86G/9.97G [01:08<00:13, 153MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  79% 7.90G/9.97G [01:08<00:12, 171MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.93G/9.97G [01:08<00:10, 187MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.96G/9.97G [01:09<00:09, 205MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 7.99G/9.97G [01:09<00:09, 215MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  80% 8.02G/9.97G [01:09<00:08, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.05G/9.97G [01:09<00:08, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.08G/9.97G [01:09<00:08, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  81% 8.12G/9.97G [01:09<00:08, 228MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.15G/9.97G [01:09<00:07, 230MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.18G/9.97G [01:10<00:10, 169MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  82% 8.21G/9.97G [01:10<00:09, 183MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.24G/9.97G [01:10<00:08, 194MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.27G/9.97G [01:10<00:08, 205MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  83% 8.30G/9.97G [01:10<00:08, 195MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.34G/9.97G [01:10<00:07, 209MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.37G/9.97G [01:11<00:07, 219MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  84% 8.40G/9.97G [01:11<00:07, 224MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.43G/9.97G [01:11<00:07, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.46G/9.97G [01:11<00:06, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  85% 8.49G/9.97G [01:11<00:06, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.52G/9.97G [01:11<00:06, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.56G/9.97G [01:11<00:06, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.59G/9.97G [01:11<00:06, 226MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  86% 8.62G/9.97G [01:12<00:05, 229MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.65G/9.97G [01:12<00:05, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.68G/9.97G [01:12<00:05, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  87% 8.71G/9.97G [01:12<00:05, 223MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.75G/9.97G [01:12<00:05, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.78G/9.97G [01:12<00:05, 225MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  88% 8.81G/9.97G [01:12<00:05, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.84G/9.97G [01:13<00:04, 231MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.87G/9.97G [01:13<00:04, 235MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  89% 8.90G/9.97G [01:13<00:04, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.93G/9.97G [01:13<00:04, 220MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 8.97G/9.97G [01:13<00:04, 210MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.00G/9.97G [01:14<00:06, 142MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  90% 9.02G/9.97G [01:17<00:42, 22.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.04G/9.97G [01:17<00:33, 27.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.07G/9.97G [01:18<00:22, 39.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  91% 9.10G/9.97G [01:18<00:16, 53.6MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.13G/9.97G [01:18<00:11, 71.1MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.15G/9.97G [01:18<00:09, 83.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.19G/9.97G [01:18<00:07, 109MB/s] \u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  92% 9.22G/9.97G [01:18<00:05, 128MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.25G/9.97G [01:18<00:04, 152MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.28G/9.97G [01:19<00:04, 169MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  93% 9.31G/9.97G [01:19<00:03, 189MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.34G/9.97G [01:19<00:03, 203MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.37G/9.97G [01:19<00:02, 214MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  94% 9.41G/9.97G [01:19<00:02, 222MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.44G/9.97G [01:19<00:02, 227MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.47G/9.97G [01:19<00:02, 234MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  95% 9.50G/9.97G [01:19<00:02, 233MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.53G/9.97G [01:20<00:01, 236MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.56G/9.97G [01:20<00:01, 242MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  96% 9.59G/9.97G [01:20<00:01, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.63G/9.97G [01:20<00:01, 243MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.66G/9.97G [01:20<00:01, 240MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  97% 9.69G/9.97G [01:20<00:01, 226MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.73G/9.97G [01:20<00:00, 249MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.76G/9.97G [01:20<00:00, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  98% 9.79G/9.97G [01:21<00:00, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.83G/9.97G [01:21<00:00, 247MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.86G/9.97G [01:23<00:03, 36.3MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin:  99% 9.89G/9.97G [01:24<00:01, 49.0MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.92G/9.97G [01:24<00:00, 64.5MB/s]\u001b[A\n",
            "pytorch_model-00001-of-00002.bin: 100% 9.97G/9.97G [01:24<00:00, 118MB/s] \n",
            "Downloading shards:  50% 1/2 [01:24<01:24, 84.61s/it]\n",
            "pytorch_model-00002-of-00002.bin:   0% 0.00/3.85G [00:00<?, ?B/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   1% 31.5M/3.85G [00:00<00:15, 241MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 62.9M/3.85G [00:00<00:16, 229MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   2% 94.4M/3.85G [00:00<00:14, 251MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   3% 126M/3.85G [00:00<00:16, 233MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   4% 157M/3.85G [00:00<00:14, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   5% 189M/3.85G [00:00<00:14, 252MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   6% 220M/3.85G [00:00<00:14, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 252M/3.85G [00:01<00:15, 239MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   7% 283M/3.85G [00:01<00:14, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   8% 315M/3.85G [00:01<00:14, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:   9% 346M/3.85G [00:01<00:14, 248MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  10% 377M/3.85G [00:01<00:14, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 409M/3.85G [00:01<00:14, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  11% 440M/3.85G [00:01<00:16, 208MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  12% 472M/3.85G [00:02<00:15, 217MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  13% 503M/3.85G [00:02<00:14, 223MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  14% 535M/3.85G [00:02<00:14, 226MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 566M/3.85G [00:02<00:16, 197MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  15% 587M/3.85G [00:02<00:20, 159MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  16% 608M/3.85G [00:05<01:58, 27.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  17% 640M/3.85G [00:05<01:21, 39.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  18% 682M/3.85G [00:05<00:52, 59.8MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 713M/3.85G [00:05<00:40, 77.1MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  19% 744M/3.85G [00:05<00:32, 96.9MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  20% 776M/3.85G [00:06<00:25, 118MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  21% 807M/3.85G [00:06<00:21, 139MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  22% 839M/3.85G [00:06<00:18, 161MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 870M/3.85G [00:06<00:16, 179MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  23% 902M/3.85G [00:06<00:15, 193MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  24% 933M/3.85G [00:06<00:14, 208MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  25% 965M/3.85G [00:06<00:13, 218MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  26% 996M/3.85G [00:07<00:12, 225MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 1.03G/3.85G [00:07<00:12, 231MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  27% 1.06G/3.85G [00:07<00:12, 231MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  28% 1.09G/3.85G [00:07<00:11, 233MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  29% 1.12G/3.85G [00:07<00:11, 234MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  30% 1.15G/3.85G [00:07<00:11, 233MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  31% 1.18G/3.85G [00:07<00:11, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.22G/3.85G [00:07<00:11, 235MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  32% 1.25G/3.85G [00:08<00:10, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  33% 1.28G/3.85G [00:08<00:10, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  34% 1.31G/3.85G [00:08<00:10, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  35% 1.34G/3.85G [00:08<00:10, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.37G/3.85G [00:08<00:10, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  36% 1.41G/3.85G [00:08<00:10, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  37% 1.44G/3.85G [00:08<00:09, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  38% 1.47G/3.85G [00:08<00:09, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  39% 1.50G/3.85G [00:09<00:09, 241MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  40% 1.53G/3.85G [00:09<00:09, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.56G/3.85G [00:09<00:09, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  41% 1.59G/3.85G [00:09<00:09, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  42% 1.63G/3.85G [00:11<00:43, 50.7MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  43% 1.67G/3.85G [00:11<00:29, 73.3MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  44% 1.70G/3.85G [00:11<00:23, 91.1MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  45% 1.73G/3.85G [00:11<00:19, 111MB/s] \u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  46% 1.76G/3.85G [00:11<00:15, 132MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.79G/3.85G [00:11<00:13, 153MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  47% 1.82G/3.85G [00:12<00:11, 172MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  48% 1.86G/3.85G [00:12<00:10, 186MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  49% 1.89G/3.85G [00:12<00:09, 206MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  50% 1.92G/3.85G [00:12<00:08, 222MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.95G/3.85G [00:12<00:08, 221MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  51% 1.98G/3.85G [00:12<00:08, 231MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  52% 2.01G/3.85G [00:12<00:07, 232MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  53% 2.04G/3.85G [00:12<00:07, 229MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  54% 2.08G/3.85G [00:13<00:07, 226MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  55% 2.11G/3.85G [00:13<00:07, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 2.14G/3.85G [00:13<00:07, 235MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  56% 2.17G/3.85G [00:13<00:07, 236MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  57% 2.20G/3.85G [00:13<00:06, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  58% 2.23G/3.85G [00:13<00:06, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  59% 2.26G/3.85G [00:13<00:06, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.30G/3.85G [00:14<00:06, 226MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  60% 2.33G/3.85G [00:14<00:06, 232MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  61% 2.36G/3.85G [00:14<00:06, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  62% 2.39G/3.85G [00:14<00:05, 252MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  63% 2.42G/3.85G [00:14<00:06, 229MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  64% 2.45G/3.85G [00:14<00:05, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.49G/3.85G [00:14<00:05, 250MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  65% 2.52G/3.85G [00:14<00:05, 249MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  66% 2.55G/3.85G [00:15<00:05, 224MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  67% 2.58G/3.85G [00:15<00:05, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  68% 2.62G/3.85G [00:15<00:04, 260MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  69% 2.65G/3.85G [00:15<00:04, 255MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.68G/3.85G [00:15<00:05, 217MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  70% 2.72G/3.85G [00:15<00:05, 199MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  71% 2.75G/3.85G [00:15<00:05, 214MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  72% 2.78G/3.85G [00:16<00:04, 223MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  73% 2.81G/3.85G [00:16<00:04, 226MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  74% 2.84G/3.85G [00:16<00:04, 233MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.87G/3.85G [00:16<00:04, 230MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  75% 2.90G/3.85G [00:16<00:04, 231MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  76% 2.94G/3.85G [00:16<00:03, 231MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  77% 2.97G/3.85G [00:16<00:03, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  78% 3.00G/3.85G [00:17<00:03, 215MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 3.03G/3.85G [00:17<00:03, 233MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  79% 3.06G/3.85G [00:17<00:03, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  80% 3.09G/3.85G [00:17<00:03, 246MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  81% 3.12G/3.85G [00:17<00:03, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  82% 3.16G/3.85G [00:17<00:02, 233MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  83% 3.19G/3.85G [00:17<00:02, 228MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 3.22G/3.85G [00:17<00:02, 229MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  84% 3.25G/3.85G [00:18<00:02, 227MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  85% 3.28G/3.85G [00:18<00:02, 240MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  86% 3.31G/3.85G [00:18<00:02, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  87% 3.34G/3.85G [00:18<00:02, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.38G/3.85G [00:18<00:01, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  88% 3.41G/3.85G [00:18<00:01, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  89% 3.44G/3.85G [00:18<00:01, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  90% 3.47G/3.85G [00:19<00:01, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  91% 3.50G/3.85G [00:19<00:01, 243MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  92% 3.53G/3.85G [00:19<00:01, 242MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.57G/3.85G [00:19<00:01, 244MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  93% 3.60G/3.85G [00:19<00:01, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  94% 3.63G/3.85G [00:19<00:00, 245MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  95% 3.66G/3.85G [00:19<00:00, 239MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  96% 3.69G/3.85G [00:19<00:00, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.72G/3.85G [00:20<00:00, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  97% 3.75G/3.85G [00:20<00:00, 238MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  98% 3.79G/3.85G [00:20<00:00, 237MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin:  99% 3.82G/3.85G [00:20<00:00, 230MB/s]\u001b[A\n",
            "pytorch_model-00002-of-00002.bin: 100% 3.85G/3.85G [00:20<00:00, 187MB/s]\n",
            "Downloading shards: 100% 2/2 [01:45<00:00, 52.73s/it]\n",
            "Loading checkpoint shards: 100% 2/2 [00:58<00:00, 29.49s/it]\n",
            "generation_config.json: 100% 121/121 [00:00<00:00, 1.06MB/s]\n",
            "\u001b[32m2025-03-18 16:34:03.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m651\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-03-18 16:34:03.710\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m656\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-03-18 16:34:03.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m669\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-03-18 16:34:03.711\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m670\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 18,739,200 || all params: 6,929,104,896 || trainable%: 0.2704\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py:658: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  warnings.warn(\n",
            "/content/MedicalGPT/pretraining.py:700: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[32m2025-03-18 16:34:04.370\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m715\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-03-18 16:34:05.038\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m716\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 49249,   1336,   3641,    923,    398,    976,    185, 100000,  17878,\n",
            "           1907,  19253,  61051,  19304,    443,    111,  92920,   1127,    398,\n",
            "          36867,   2266,   1680,  19304,   1671,  23639,  10741,   7560,    913,\n",
            "          19304,  67408,  33433,    398,   7664,  46964,   3160,   1759,  45326,\n",
            "            790,   1372,   4045,   1720,  52055,  17497,   2224,    976,   4200,\n",
            "          15839,   1759,  45326,    790,    852,   6074,  17497,  19304,  19015,\n",
            "           6342,    885,  50217,  50217,   1506,    929,   1748,  65510,  19304,\n",
            "           2830,  32845,   8375,    398,   2141,   8438,  76780,   1372,  51433,\n",
            "           7861,   7351,    398,    976,   7664,  46964,   3160,  24040,  13147,\n",
            "          84064,   6622,    504,   6214,  19304,   1759,  45326,    790,  10012,\n",
            "           1372,   1087,   1827,  13725,    398,  32508,  26273,  17878,   9457,\n",
            "          17699,  49373,  19304,  54029,    573,  99880,   3411,   2104,   2224,\n",
            "          36397,    787,  70560,    923,  24437,   1827,   2224,    976,   4200,\n",
            "          15839,   1759,  45326,    790,   4607,   2510,   2125,  14847,   3381,\n",
            "           9554,    537],\n",
            "        [  1848,   4276,   1507,   3672,  29371,   1662,    106,  37559,   1186,\n",
            "           1759,  19304,   4595,   1186,  33965,  37163,  20753,  19304,  22924,\n",
            "           1391,  77338,    612,  37163,   5808,   8245,  21083,    398,  19388,\n",
            "           1087,   1848,   1759,  45326,    790,  78942,   2961,   2412,    398,\n",
            "            976,    185, 100000,   4200,   2206,  65208,   1759,  45326,    790,\n",
            "          17374,   2160,   1372,   8010,  19472,   1537,    852,    398,    976,\n",
            "           2206,    892,   2657,   1186,   1759,    764,  25468,   1827,  19304,\n",
            "          16127,   3344,   2797,   4090,  91574,  78500,   3115,    505,  19370,\n",
            "          19304,   2108,   1511,   6622,    504,  16920,    398,   4200,   2206,\n",
            "          65208,  21846,  20239,   1762,  19304,  15710,   1848,  10564,  19304,\n",
            "           3846,   5093,   9066,   1759,  45326,    790,   3149,    630,   2224,\n",
            "            976,    185, 100000,  91574,   1102,   1873,    630,   3846,   5093,\n",
            "           1759,  45326,    790,   3996,  14714,   2160,  26224,  19304,  46826,\n",
            "           4547,  15315,    398,    976,  28471,   1649,  86910,   8375,    398,\n",
            "           4200,   2206],\n",
            "        [ 50836,   1854,   4779,   1965,    121,   3056,  96102,    398,   2775,\n",
            "           3056,  20631,  59241,  12295,  19304,  10435,    537,  63103,   3056,\n",
            "            573,  75993,  34126,  19304,  32101,  50836,  23212,   3056,  96102,\n",
            "           4425,     16,     15,    948,    214,   3611,  34126,  19304,  38966,\n",
            "           3056,   9467,     20,     15,      4,  19304,  15544,   3056,   4425,\n",
            "             22,     15,    948,    214,  75993,  34126,    398,    185, 100000,\n",
            "          50836,   4779,   1965,    121,   3056,  96102,  45326,  24364,  10244,\n",
            "            337,  82562,  27455,  10239,  19304,  90091,   1937,   2045,    398,\n",
            "           4779,   1965,    121,  28315,  45326,  10435,    537,  63103,   3056,\n",
            "          96102,   1762,   3175,   3310,   8092,  19304,   4425,     23,     15,\n",
            "            948,    214,   9752,    609,  48763,    537,   9093,  33770,  11527,\n",
            "            398,  23212,    537,  38966,    537,  15544,   3056,  96102,   1762,\n",
            "           3175,  14130,   3310,  11165,  19304,   4225,  31394,    609,  48763,\n",
            "            398,   2133,   2045,   5649,  16323,    609,  29587,  75471,    537,\n",
            "          53636,  89418]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 49249,   1336,   3641,    923,    398,    976,    185, 100000,  17878,\n",
            "           1907,  19253,  61051,  19304,    443,    111,  92920,   1127,    398,\n",
            "          36867,   2266,   1680,  19304,   1671,  23639,  10741,   7560,    913,\n",
            "          19304,  67408,  33433,    398,   7664,  46964,   3160,   1759,  45326,\n",
            "            790,   1372,   4045,   1720,  52055,  17497,   2224,    976,   4200,\n",
            "          15839,   1759,  45326,    790,    852,   6074,  17497,  19304,  19015,\n",
            "           6342,    885,  50217,  50217,   1506,    929,   1748,  65510,  19304,\n",
            "           2830,  32845,   8375,    398,   2141,   8438,  76780,   1372,  51433,\n",
            "           7861,   7351,    398,    976,   7664,  46964,   3160,  24040,  13147,\n",
            "          84064,   6622,    504,   6214,  19304,   1759,  45326,    790,  10012,\n",
            "           1372,   1087,   1827,  13725,    398,  32508,  26273,  17878,   9457,\n",
            "          17699,  49373,  19304,  54029,    573,  99880,   3411,   2104,   2224,\n",
            "          36397,    787,  70560,    923,  24437,   1827,   2224,    976,   4200,\n",
            "          15839,   1759,  45326,    790,   4607,   2510,   2125,  14847,   3381,\n",
            "           9554,    537],\n",
            "        [  1848,   4276,   1507,   3672,  29371,   1662,    106,  37559,   1186,\n",
            "           1759,  19304,   4595,   1186,  33965,  37163,  20753,  19304,  22924,\n",
            "           1391,  77338,    612,  37163,   5808,   8245,  21083,    398,  19388,\n",
            "           1087,   1848,   1759,  45326,    790,  78942,   2961,   2412,    398,\n",
            "            976,    185, 100000,   4200,   2206,  65208,   1759,  45326,    790,\n",
            "          17374,   2160,   1372,   8010,  19472,   1537,    852,    398,    976,\n",
            "           2206,    892,   2657,   1186,   1759,    764,  25468,   1827,  19304,\n",
            "          16127,   3344,   2797,   4090,  91574,  78500,   3115,    505,  19370,\n",
            "          19304,   2108,   1511,   6622,    504,  16920,    398,   4200,   2206,\n",
            "          65208,  21846,  20239,   1762,  19304,  15710,   1848,  10564,  19304,\n",
            "           3846,   5093,   9066,   1759,  45326,    790,   3149,    630,   2224,\n",
            "            976,    185, 100000,  91574,   1102,   1873,    630,   3846,   5093,\n",
            "           1759,  45326,    790,   3996,  14714,   2160,  26224,  19304,  46826,\n",
            "           4547,  15315,    398,    976,  28471,   1649,  86910,   8375,    398,\n",
            "           4200,   2206],\n",
            "        [ 50836,   1854,   4779,   1965,    121,   3056,  96102,    398,   2775,\n",
            "           3056,  20631,  59241,  12295,  19304,  10435,    537,  63103,   3056,\n",
            "            573,  75993,  34126,  19304,  32101,  50836,  23212,   3056,  96102,\n",
            "           4425,     16,     15,    948,    214,   3611,  34126,  19304,  38966,\n",
            "           3056,   9467,     20,     15,      4,  19304,  15544,   3056,   4425,\n",
            "             22,     15,    948,    214,  75993,  34126,    398,    185, 100000,\n",
            "          50836,   4779,   1965,    121,   3056,  96102,  45326,  24364,  10244,\n",
            "            337,  82562,  27455,  10239,  19304,  90091,   1937,   2045,    398,\n",
            "           4779,   1965,    121,  28315,  45326,  10435,    537,  63103,   3056,\n",
            "          96102,   1762,   3175,   3310,   8092,  19304,   4425,     23,     15,\n",
            "            948,    214,   9752,    609,  48763,    537,   9093,  33770,  11527,\n",
            "            398,  23212,    537,  38966,    537,  15544,   3056,  96102,   1762,\n",
            "           3175,  14130,   3310,  11165,  19304,   4225,  31394,    609,  48763,\n",
            "            398,   2133,   2045,   5649,  16323,    609,  29587,  75471,    537,\n",
            "          53636,  89418]], device='cuda:0')}\u001b[0m\n",
            "{'loss': 2.9201, 'grad_norm': 0.41207155585289, 'learning_rate': 4.444444444444445e-06, 'epoch': 0.0}\n",
            "{'loss': 2.7201, 'grad_norm': 0.45105651021003723, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.01}\n",
            "{'loss': 2.9258, 'grad_norm': 0.4875084161758423, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.02}\n",
            "{'loss': 2.6475, 'grad_norm': 0.9869950413703918, 'learning_rate': 0.00013333333333333334, 'epoch': 0.03}\n",
            "{'loss': 2.5974, 'grad_norm': 0.817748486995697, 'learning_rate': 0.00017777777777777779, 'epoch': 0.05}\n",
            "{'loss': 2.6202, 'grad_norm': 0.9691812992095947, 'learning_rate': 0.00019880525686977302, 'epoch': 0.06}\n",
            "  6% 50/882 [05:49<1:41:11,  7.30s/it]Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "\n",
            "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            " 50% 2/4 [00:02<00:02,  1.13s/it]\u001b[A\n",
            " 75% 3/4 [00:04<00:01,  1.61s/it]\u001b[A\n",
            "                                      \n",
            "\u001b[A{'eval_loss': 2.321094512939453, 'eval_accuracy': 0.5448818897637795, 'eval_runtime': 7.7423, 'eval_samples_per_second': 1.292, 'eval_steps_per_second': 0.517, 'epoch': 0.06}\n",
            "  6% 50/882 [05:57<1:41:11,  7.30s/it]\n",
            "100% 4/4 [00:05<00:00,  1.32s/it]\u001b[A\n",
            "{'loss': 2.5417, 'grad_norm': 0.8770740032196045, 'learning_rate': 0.00019641577060931903, 'epoch': 0.07}\n",
            "{'loss': 2.4699, 'grad_norm': 0.719561755657196, 'learning_rate': 0.000194026284348865, 'epoch': 0.08}\n",
            "{'loss': 2.4401, 'grad_norm': 0.7502880096435547, 'learning_rate': 0.000191636798088411, 'epoch': 0.09}\n",
            " 10% 86/882 [10:21<1:37:54,  7.38s/it]Traceback (most recent call last):\n",
            "  File \"/content/MedicalGPT/pretraining.py\", line 759, in <module>\n",
            "    main()\n",
            "  File \"/content/MedicalGPT/pretraining.py\", line 720, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2241, in train\n",
            "    return inner_training_loop(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 2548, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\", line 3740, in training_step\n",
            "    self.accelerator.backward(loss, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py\", line 2246, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            " 10% 86/882 [10:28<1:37:00,  7.31s/it]\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python pretraining.py \\\n",
        "    --model_name_or_path deepseek-ai/deepseek-llm-7b-base \\\n",
        "    --train_file_dir ./data/pretrain \\\n",
        "    --validation_file_dir ./data/pretrain \\\n",
        "    --per_device_train_batch_size 3 \\\n",
        "    --per_device_eval_batch_size 3 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 20000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-4 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --evaluation_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --block_size 128 \\\n",
        "    --group_by_length True \\\n",
        "    --output_dir outputs-pt-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2dl19S-AIb0"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-pt-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QnS9RVTAIb1"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GPwmOFzLAIb2"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdCQyzljAIb2"
      },
      "outputs": [],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model Qwen/Qwen2.5-0.5B --lora_model outputs-pt-v1 --output_dir merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImrXhWqtAIb2"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-pt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yFZMf8nAIb3"
      },
      "outputs": [],
      "source": [
        "%cat merged-pt/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBu6XX3nAIb3"
      },
      "source": [
        "Stage1 增量预训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:56:17.081153Z",
          "start_time": "2023-06-15T13:56:17.032821Z"
        },
        "id": "pdat-ANgAIb3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "6rB3W22cAIb3"
      },
      "source": [
        "# Stage 2: Supervised FineTuning\n",
        "\n",
        "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
        "\n",
        "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "v3smO2b0AIb4"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage1得到的预训练模型\n",
        "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3J_51O20AIb4"
      },
      "source": [
        "## Stage2 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T13:58:38.966506Z",
          "start_time": "2023-06-15T13:58:38.778132Z"
        },
        "id": "Jqz9ZwfaAIb4",
        "outputId": "d6a91c3a-4597-4d22-c71d-349d758a1eb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "medical_sft_1K_format.jsonl  sharegpt_zh_1K_format.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/finetune"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aQ96YO9JLxab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "oXaSG2VCLVjJ",
        "outputId": "0f7df8f6-6e9f-4af3-e4ec-cc3782950e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Ananlia` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Ananlia`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YTgW90m0AIb5",
        "outputId": "323c2339-4ba6-4d51-d8ae-416df2a98e6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-18 17:02:38.514005: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742317358.544369   14623 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742317358.550842   14623 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-18 17:02:38.572134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[32m2025-03-18 17:02:41.652\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m192\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-03-18 17:02:41.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m309\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='bfloat16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:41.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m310\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:41.888\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m311\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=True,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-sft-v1/runs/Mar18_17-02-41_5e53693f8c97,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-sft-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=4,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-sft-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.05,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:41.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m312\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:41.889\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m313\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "tokenizer_config.json: 100% 3.07k/3.07k [00:00<00:00, 15.7MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 15.4MB/s]\n",
            "\u001b[32m2025-03-18 17:02:44.827\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m346\u001b[0m - \u001b[34m\u001b[1mTokenizer: LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:44.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m374\u001b[0m - \u001b[1mtrain files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:44.828\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m379\u001b[0m - \u001b[1meval files: ['./data/finetune/medical_sft_1K_format.jsonl', './data/finetune/sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
            "Generating train split: 2000 examples [00:00, 17395.09 examples/s]\n",
            "Generating validation split: 2000 examples [00:00, 63873.78 examples/s]\n",
            "\u001b[32m2025-03-18 17:02:45.721\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m395\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['conversations'],\n",
            "        num_rows: 2000\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:45.739\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m497\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '做广告者。'}, {'from': 'gpt', 'value': '当然。作为广告客户，我如何帮助您？您广告活动的目标是什么？您的目标受众是什么？请提供更多信息，以便我能够为您提供更有针对性的支持。'}, {'from': 'human', 'value': '请为我找到FOMO的替代概念，以便我将其用作广告。\\n例如，我为某个受众利用FOMO来创造类似“我能排除某人吗？”或“我能变得有阶级意识吗？”等概念。\\n您需要为我找到FOMO的其他替代概念，这将对我创建广告内容很有帮助。'}, {'from': 'gpt', 'value': '以下是一些你可以用于广告活动的替代概念：\\n1. 紧迫性：在优惠活动周围创造一种紧迫感，让人们感到必须立刻行动，以免错失机会。\\n2. 独家性：让人们感到他们正在购买独特的东西，而这些东西不是每个人都可以得到的。\\n3. 稀缺性：在促销活动周围创造一种稀缺感，让人们感到可用的产品数量有限。\\n4. 新鲜感：强调产品或服务是新的和创新的，以及如何为人们的生活带来积极变化。\\n5. 独特好处：突出产品或服务的独特好处，以及如何解决你目标受众的特定问题。\\n这些只是一些你可以用于广告活动的替代概念的例子。重要的是你要了解你目标受众并能够创建一个与他们共鸣的信息。'}, {'from': 'human', 'value': 'Benissimo，列出其他10个概念。'}, {'from': 'gpt', 'value': '以下是另外10个可供广告活动使用的替代概念：\\n1. 便利性：突出产品或服务可以使人们的生活更简单、更方便。\\n2. 质量：强调产品或服务的质量，并说明它如何与竞争对手相比有所区别。\\n3. 流行度：让购买者感受到他们正在购买受欢迎的物品，有很多其他人已经选择了这个产品。\\n4. 节约：强调产品或服务可以帮助节省金钱或时间。\\n5. 个性化：强调产品或服务可以根据人们的具体需求进行个性化定制。\\n6. 满意保证：强调您的满意保证政策，并说明这可以给购买者带来安心，如果不满意可以退货。\\n7. 支持：强调您在销售前后提供的支持。\\n8. 透明度：强调您的透明度，在报价方面让人们感觉他们确切知道自己正在购买什么。\\n9. 趋势：突显产品或服务与当前趋势保持一致，并说明它如何成为人们生活中的时尚附加品。\\n10. 价值：强调产品或服务的价值，证明购买者可以花费相应的价格获得很多物品。\\n这些都是另外10个替代概念，可供广告活动使用。希望这些可以为你提供新的灵感，让你更好地创建下一个广告内容。'}]}\u001b[0m\n",
            "Running tokenizer on dataset: 100% 1000/1000 [00:01<00:00, 701.31 examples/s]\n",
            "Filter: 100% 998/998 [00:00<00:00, 2823.48 examples/s]\n",
            "\u001b[32m2025-03-18 17:02:47.637\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m514\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 998\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:47.637\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m515\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:47.640\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m516\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "<｜begin▁of▁sentence｜>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 做广告者。 ASSISTANT:当然。作为广告客户，我如何帮助您？您广告活动的目标是什么？您的目标受众是什么？请提供更多信息，以便我能够为您提供更有针对性的支持。<｜end▁of▁sentence｜></s>USER: 请为我找到FOMO的替代概念，以便我将其用作广告。\n",
            "例如，我为某个受众利用FOMO来创造类似“我能排除某人吗？”或“我能变得有阶级意识吗？”等概念。\n",
            "您需要为我找到FOMO的其他替代概念，这将对我创建广告内容很有帮助。 ASSISTANT:以下是一些你可以用于广告活动的替代概念：\n",
            "1. 紧迫性：在优惠活动周围创造一种紧迫感，让人们感到必须立刻行动，以免错失机会。\n",
            "2. 独家性：让人们感到他们正在购买独特的东西，而这些东西不是每个人都可以得到的。\n",
            "3. 稀缺性：在促销活动周围创造一种稀缺感，让人们感到可用的产品数量有限。\n",
            "4. 新鲜感：强调产品或服务是新的和创新的，以及如何为人们的生活带来积极变化。\n",
            "5. 独特好处：突出产品或服务的独特好处，以及如何解决你目标受众的特定问题。\n",
            "这些只是一些你可以用于广告活动的替代概念的例子。重要的是你要了解你目标受众并能够创建一个与他们共鸣的信息。<｜end▁of▁sentence｜>\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:47.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m519\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜>当然。作为广告客户，我如何帮助您？您广告活动的目标是什么？您的目标受众是什么？请提供更多信息，以便我能够为您提供更有针对性的支持。<｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜>以下是一些你可以用于广告活动的替代概念：\n",
            "1. 紧迫性：在优惠活动周围创造一种紧迫感，让人们感到必须立刻行动，以免错失机会。\n",
            "2. 独家性：让人们感到他们正在购买独特的东西，而这些东西不是每个人都可以得到的。\n",
            "3. 稀缺性：在促销活动周围创造一种稀缺感，让人们感到可用的产品数量有限。\n",
            "4. 新鲜感：强调产品或服务是新的和创新的，以及如何为人们的生活带来积极变化。\n",
            "5. 独特好处：突出产品或服务的独特好处，以及如何解决你目标受众的特定问题。\n",
            "这些只是一些你可以用于广告活动的替代概念的例子。重要的是你要了解你目标受众并能够创建一个与他们共鸣的信息。<｜end▁of▁sentence｜>\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:47.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m533\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:47.645\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m537\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？'}, {'from': 'gpt', 'value': '男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。'}]}\u001b[0m\n",
            "Running tokenizer on validation dataset: 100% 10/10 [00:00<00:00, 698.12 examples/s]\n",
            "Filter: 100% 10/10 [00:00<00:00, 1990.27 examples/s]\n",
            "\u001b[32m2025-03-18 17:02:47.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m547\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:47.796\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-03-18 17:02:47.798\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[34m\u001b[1m<｜begin▁of▁sentence｜>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？ ASSISTANT:男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。<｜end▁of▁sentence｜>\u001b[0m\n",
            "config.json: 100% 679/679 [00:00<00:00, 4.79MB/s]\n",
            "model.safetensors: 100% 3.55G/3.55G [00:23<00:00, 151MB/s]\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.11MB/s]\n",
            "\u001b[32m2025-03-18 17:03:17.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m688\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-03-18 17:03:17.539\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m703\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-03-18 17:03:17.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m712\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-03-18 17:03:17.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m713\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 9,232,384 || all params: 1,786,320,384 || trainable%: 0.5168\n",
            "\u001b[32m2025-03-18 17:03:18.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m735\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
            "/content/MedicalGPT/supervised_finetuning.py:752: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SavePeftModelTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SavePeftModelTrainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[32m2025-03-18 17:03:18.094\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m764\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "\u001b[32m2025-03-18 17:03:18.115\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m766\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[151643, 151643, 151643,  ..., 101953, 101899, 151643],\n",
            "        [151646,     32,   6236,  ...,  99898,   1773, 151643],\n",
            "        [151643, 151643, 151643,  ..., 100166,   1773, 151643],\n",
            "        [151643, 151643, 151643,  ..., 110162,  49567, 151643]],\n",
            "       device='cuda:0'), 'attention_mask': tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1],\n",
            "        [0, 0, 0,  ..., 1, 1, 1]], device='cuda:0'), 'labels': tensor([[  -100,   -100,   -100,  ..., 101953, 101899, 151643],\n",
            "        [  -100,   -100,   -100,  ...,  99898,   1773, 151643],\n",
            "        [  -100,   -100,   -100,  ..., 100166,   1773, 151643],\n",
            "        [  -100,   -100,   -100,  ..., 110162,  49567, 151643]],\n",
            "       device='cuda:0')}\u001b[0m\n",
            "\u001b[32m2025-03-18 17:03:18.155\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m767\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
            "[tensor([151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151646,     32,   6236,   1948,    264,  22208,   1196,    323,    458,\n",
            "         20443,  11229,  17847,     13,    576,  17847,   6696,  10950,     11,\n",
            "         11682,     11,    323,  47787,  11253,    311,    279,   1196,    594,\n",
            "          4755,   3918,     82,     29,   6448,     25,  18137,    103,    101,\n",
            "         32108,  33071,  99180,  35551,  45356,  99180,  35551,  99252,   9370,\n",
            "        104650, 101899, 101895,  99245,  11319,  35560,   3846,   2821,     25,\n",
            "         32664,  99769, 100143,  54542,  24968, 120412,  99180, 101953, 101899,\n",
            "        151643], device='cuda:0'), tensor([151646,     32,   6236,   1948,    264,  22208,   1196,    323,    458,\n",
            "         20443,  11229,  17847,     13,    576,  17847,   6696,  10950,     11,\n",
            "         11682,     11,    323,  47787,  11253,    311,    279,   1196,    594,\n",
            "          4755,   3918,     82,     29,   6448,     25,    220, 107809,  11622,\n",
            "         25411,  40814, 101454,  24339, 112672, 100625,  28404,  99678,   9370,\n",
            "        114091, 101037,  11319,  35560,   3846,   2821,     25, 103942,  73670,\n",
            "             0,  32181,    247,  20412, 100625,  28404,  99678,  26940,  77419,\n",
            "         38989,  99858,  25067,   9370, 114091,   3837,  59879,  19360,  51461,\n",
            "           229,  40814,  51463,  48443,  73594,  12669,    198,     55,     25,\n",
            "            16,    198,     51,  74045,   7679,   6222,     79,  38940,  39614,\n",
            "           198,     44,     25,     19,     14,     19,    198,     43,     25,\n",
            "            16,     14,     19,    198,     42,  69856,    198,     48,     25,\n",
            "            16,     14,     19,     28,     16,     17,     15,    198,     89,\n",
            "            17,    760,    434,     17,    434,     17,    362,     17,    272,\n",
            "            17,    760,    272,     17,    272,     17,    425,     17,    362,\n",
            "            17,    760,    479,     17,    479,     17,    479,     17,    362,\n",
            "            17,    760,    425,     17,    425,     17,    425,     17,   1147,\n",
            "            17,   9248,     66,      6,     17,    272,      6,     17,    294,\n",
            "             6,     17,    384,      6,     17,    760,    282,      6,     17,\n",
            "           282,      6,     17,    384,      6,     17,    294,      6,     17,\n",
            "           760,    272,      6,     17,    272,      6,     17,    425,     17,\n",
            "           362,     17,    760,    479,     17,    479,     17,    479,     17,\n",
            "          1147,     17,   9248,     89,     17,    760,    272,     17,    272,\n",
            "            17,    294,     17,    384,     17,    760,    282,     17,    282,\n",
            "            17,    384,     17,    294,     17,    760,    272,     17,    272,\n",
            "            17,    425,     17,    362,     17,    760,    479,     17,    479,\n",
            "            17,    479,     17,   1147,     17,   9248,     89,     17,    760,\n",
            "           434,     17,    434,     17,    362,     17,    272,     17,    760,\n",
            "           272,     17,    272,     17,    425,     17,    362,     17,    760,\n",
            "           479,     17,    479,     17,    479,     17,    362,     17,    760,\n",
            "           425,     17,    425,     17,    425,     17,   1147,     17,   9248,\n",
            "            66,      6,     17,    272,      6,     17,    294,      6,     17,\n",
            "           384,      6,     17,    760,    282,      6,     17,    282,      6,\n",
            "            17,    384,     17,    294,     17,    760,    272,      6,     17,\n",
            "           272,      6,     17,    425,     17,    362,     17,    760,    479,\n",
            "            17,    479,     17,    479,     17,   1147,     17,   9248,  13874,\n",
            "         19324, 100137,  40814, 101454,  24339, 116984,  44063, 114091,  31196,\n",
            "         26939, 100646, 103951,  74220,  15946,   3837, 101912,  99350, 101454,\n",
            "        103951,  57191, 106726,  31548,   3837, 105920, 114091,  73670,  18397,\n",
            "         53222,  57191,  23031, 101454,  20742, 100414, 102703,  99898,   1773,\n",
            "        151643], device='cuda:0'), tensor([151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "        151646,     32,   6236,   1948,    264,  22208,   1196,    323,    458,\n",
            "         20443,  11229,  17847,     13,    576,  17847,   6696,  10950,     11,\n",
            "         11682,     11,    323,  47787,  11253,    311,    279,   1196,    594,\n",
            "          4755,   3918,     82,     29,   6448,     25,  69372,  98749,  98237,\n",
            "         30534, 106637,  35560,   3846,   2821,     25, 106637, 101158, 101042,\n",
            "        108872,  51575, 105080,   3837, 102215,  99795, 102064,   5373, 104564,\n",
            "        102064,  99998,  20074, 100166,   3837,  71268, 101137, 100414,  72881,\n",
            "        102648,  46448,   1773, 106637, 104820,  20412,  60610,  31196,   9370,\n",
            "        100166,   3837, 102119,  17714,  99794,  68862,  31196,  57191,  99553,\n",
            "        102988, 109963,  66017,   8997, 106637,  66558, 118619,  75768,   3837,\n",
            "        100398,  37029, 104339, 108747,  96555, 106637, 106708, 102064,  33108,\n",
            "        109988,  66017,   1773, 101883, 102716, 106637,  99361, 100630,    510,\n",
            "             9,  61991,  99743, 111228, 106637,     25,    220,  75882,  39907,\n",
            "         45181,  31196,   9370, 102198, 100166,  55286,   3837, 104137, 101042,\n",
            "        103991, 108872,   8997,      9,  61991,  99413, 105798, 106637,     25,\n",
            "           220,  75882,  39907,  45181, 106506, 108872,  55286,   3837, 105798,\n",
            "        101042, 101908,  31196, 100166,   8997,      9,    220, 100520, 100040,\n",
            "        102008, 106637,     25,  32181,    247,  86402,  39907,  37029, 108940,\n",
            "        100520, 100040,  32804,  36407, 101042,  31196,   9370, 100166,   1773,\n",
            "        151643], device='cuda:0')], \n",
            "labels:\n",
            "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "         32664,  99769, 100143,  54542,  24968, 120412,  99180, 101953, 101899,\n",
            "        151643], device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100, 103942,  73670,\n",
            "             0,  32181,    247,  20412, 100625,  28404,  99678,  26940,  77419,\n",
            "         38989,  99858,  25067,   9370, 114091,   3837,  59879,  19360,  51461,\n",
            "           229,  40814,  51463,  48443,  73594,  12669,    198,     55,     25,\n",
            "            16,    198,     51,  74045,   7679,   6222,     79,  38940,  39614,\n",
            "           198,     44,     25,     19,     14,     19,    198,     43,     25,\n",
            "            16,     14,     19,    198,     42,  69856,    198,     48,     25,\n",
            "            16,     14,     19,     28,     16,     17,     15,    198,     89,\n",
            "            17,    760,    434,     17,    434,     17,    362,     17,    272,\n",
            "            17,    760,    272,     17,    272,     17,    425,     17,    362,\n",
            "            17,    760,    479,     17,    479,     17,    479,     17,    362,\n",
            "            17,    760,    425,     17,    425,     17,    425,     17,   1147,\n",
            "            17,   9248,     66,      6,     17,    272,      6,     17,    294,\n",
            "             6,     17,    384,      6,     17,    760,    282,      6,     17,\n",
            "           282,      6,     17,    384,      6,     17,    294,      6,     17,\n",
            "           760,    272,      6,     17,    272,      6,     17,    425,     17,\n",
            "           362,     17,    760,    479,     17,    479,     17,    479,     17,\n",
            "          1147,     17,   9248,     89,     17,    760,    272,     17,    272,\n",
            "            17,    294,     17,    384,     17,    760,    282,     17,    282,\n",
            "            17,    384,     17,    294,     17,    760,    272,     17,    272,\n",
            "            17,    425,     17,    362,     17,    760,    479,     17,    479,\n",
            "            17,    479,     17,   1147,     17,   9248,     89,     17,    760,\n",
            "           434,     17,    434,     17,    362,     17,    272,     17,    760,\n",
            "           272,     17,    272,     17,    425,     17,    362,     17,    760,\n",
            "           479,     17,    479,     17,    479,     17,    362,     17,    760,\n",
            "           425,     17,    425,     17,    425,     17,   1147,     17,   9248,\n",
            "            66,      6,     17,    272,      6,     17,    294,      6,     17,\n",
            "           384,      6,     17,    760,    282,      6,     17,    282,      6,\n",
            "            17,    384,     17,    294,     17,    760,    272,      6,     17,\n",
            "           272,      6,     17,    425,     17,    362,     17,    760,    479,\n",
            "            17,    479,     17,    479,     17,   1147,     17,   9248,  13874,\n",
            "         19324, 100137,  40814, 101454,  24339, 116984,  44063, 114091,  31196,\n",
            "         26939, 100646, 103951,  74220,  15946,   3837, 101912,  99350, 101454,\n",
            "        103951,  57191, 106726,  31548,   3837, 105920, 114091,  73670,  18397,\n",
            "         53222,  57191,  23031, 101454,  20742, 100414, 102703,  99898,   1773,\n",
            "        151643], device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
            "          -100,   -100,   -100,   -100,   -100,   -100, 106637, 101158, 101042,\n",
            "        108872,  51575, 105080,   3837, 102215,  99795, 102064,   5373, 104564,\n",
            "        102064,  99998,  20074, 100166,   3837,  71268, 101137, 100414,  72881,\n",
            "        102648,  46448,   1773, 106637, 104820,  20412,  60610,  31196,   9370,\n",
            "        100166,   3837, 102119,  17714,  99794,  68862,  31196,  57191,  99553,\n",
            "        102988, 109963,  66017,   8997, 106637,  66558, 118619,  75768,   3837,\n",
            "        100398,  37029, 104339, 108747,  96555, 106637, 106708, 102064,  33108,\n",
            "        109988,  66017,   1773, 101883, 102716, 106637,  99361, 100630,    510,\n",
            "             9,  61991,  99743, 111228, 106637,     25,    220,  75882,  39907,\n",
            "         45181,  31196,   9370, 102198, 100166,  55286,   3837, 104137, 101042,\n",
            "        103991, 108872,   8997,      9,  61991,  99413, 105798, 106637,     25,\n",
            "           220,  75882,  39907,  45181, 106506, 108872,  55286,   3837, 105798,\n",
            "        101042, 101908,  31196, 100166,   8997,      9,    220, 100520, 100040,\n",
            "        102008, 106637,     25,  32181,    247,  86402,  39907,  37029, 108940,\n",
            "        100520, 100040,  32804,  36407, 101042,  31196,   9370, 100166,   1773,\n",
            "        151643], device='cuda:0')]\u001b[0m\n",
            "\u001b[32m2025-03-18 17:03:18.181\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m768\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
            "<｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜begin▁of▁sentence｜>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 骨化性气管支气管病的辅助治疗有些什么？ ASSISTANT:对症支持处理；氩气刀治疗<｜end▁of▁sentence｜>\u001b[0m\n",
            "\u001b[32m2025-03-18 17:03:18.326\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m771\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
            "<｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜><｜end▁of▁sentence｜>对症支持处理；氩气刀治疗<｜end▁of▁sentence｜>\u001b[0m\n",
            "{'loss': 3.4022, 'grad_norm': 0.8456529378890991, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\n",
            "{'loss': 3.3647, 'grad_norm': 0.4965590536594391, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.04}\n",
            "{'loss': 3.6234, 'grad_norm': 1.090477705001831, 'learning_rate': 1.9409282700421944e-05, 'epoch': 0.08}\n",
            "{'loss': 3.3364, 'grad_norm': 0.6400810480117798, 'learning_rate': 1.856540084388186e-05, 'epoch': 0.12}\n",
            "{'loss': 3.1573, 'grad_norm': 0.8033381700515747, 'learning_rate': 1.7721518987341772e-05, 'epoch': 0.16}\n",
            "{'loss': 3.3715, 'grad_norm': 0.7424730658531189, 'learning_rate': 1.687763713080169e-05, 'epoch': 0.2}\n",
            " 20% 50/250 [06:18<28:59,  8.70s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:02<00:01,  1.19s/it]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 4.567862510681152, 'eval_runtime': 6.0488, 'eval_samples_per_second': 1.653, 'eval_steps_per_second': 0.496, 'epoch': 0.2}\n",
            " 20% 50/250 [06:24<28:59,  8.70s/it]\n",
            "100% 3/3 [00:05<00:00,  1.60s/it]\u001b[A\n",
            "{'loss': 3.576, 'grad_norm': 0.8863565325737, 'learning_rate': 1.6033755274261603e-05, 'epoch': 0.24}\n",
            "{'loss': 3.4554, 'grad_norm': 0.7189267873764038, 'learning_rate': 1.5189873417721521e-05, 'epoch': 0.28}\n",
            "{'loss': 3.198, 'grad_norm': 0.6507024168968201, 'learning_rate': 1.4345991561181437e-05, 'epoch': 0.32}\n",
            "{'loss': 2.9968, 'grad_norm': 0.7932102680206299, 'learning_rate': 1.350210970464135e-05, 'epoch': 0.36}\n",
            "{'loss': 3.0513, 'grad_norm': 0.7772051095962524, 'learning_rate': 1.2658227848101268e-05, 'epoch': 0.4}\n",
            " 40% 100/250 [13:06<21:04,  8.43s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:02<00:01,  1.19s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.360468864440918, 'eval_runtime': 6.0456, 'eval_samples_per_second': 1.654, 'eval_steps_per_second': 0.496, 'epoch': 0.4}\n",
            " 40% 100/250 [13:12<21:04,  8.43s/it]\n",
            "100% 3/3 [00:05<00:00,  1.59s/it]\u001b[A\n",
            "{'loss': 3.0608, 'grad_norm': 1.256365180015564, 'learning_rate': 1.1814345991561182e-05, 'epoch': 0.44}\n",
            "{'loss': 3.1507, 'grad_norm': 0.8239213228225708, 'learning_rate': 1.0970464135021096e-05, 'epoch': 0.48}\n",
            "{'loss': 2.7946, 'grad_norm': 1.105361819267273, 'learning_rate': 1.0126582278481014e-05, 'epoch': 0.52}\n",
            "{'loss': 2.9203, 'grad_norm': 0.9101645350456238, 'learning_rate': 9.28270042194093e-06, 'epoch': 0.56}\n",
            "{'loss': 2.859, 'grad_norm': 0.7477586269378662, 'learning_rate': 8.438818565400846e-06, 'epoch': 0.6}\n",
            " 60% 150/250 [19:38<11:45,  7.05s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:02<00:01,  1.20s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.3128342628479, 'eval_runtime': 6.1072, 'eval_samples_per_second': 1.637, 'eval_steps_per_second': 0.491, 'epoch': 0.6}\n",
            " 60% 150/250 [19:44<11:45,  7.05s/it]\n",
            "100% 3/3 [00:05<00:00,  1.61s/it]\u001b[A\n",
            "{'loss': 3.0153, 'grad_norm': 0.7254159450531006, 'learning_rate': 7.5949367088607605e-06, 'epoch': 0.64}\n",
            "{'loss': 2.8323, 'grad_norm': 0.6179220676422119, 'learning_rate': 6.751054852320675e-06, 'epoch': 0.68}\n",
            "{'loss': 3.0397, 'grad_norm': 0.6511217951774597, 'learning_rate': 5.907172995780591e-06, 'epoch': 0.72}\n",
            "{'loss': 2.8137, 'grad_norm': 0.5921759009361267, 'learning_rate': 5.063291139240507e-06, 'epoch': 0.76}\n",
            "{'loss': 3.0348, 'grad_norm': 0.7604183554649353, 'learning_rate': 4.219409282700423e-06, 'epoch': 0.8}\n",
            " 80% 200/250 [26:35<06:50,  8.21s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:02<00:01,  1.20s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.299993991851807, 'eval_runtime': 6.1167, 'eval_samples_per_second': 1.635, 'eval_steps_per_second': 0.49, 'epoch': 0.8}\n",
            " 80% 200/250 [26:41<06:50,  8.21s/it]\n",
            "100% 3/3 [00:05<00:00,  1.61s/it]\u001b[A\n",
            "{'loss': 3.0196, 'grad_norm': 0.8959814310073853, 'learning_rate': 3.3755274261603377e-06, 'epoch': 0.84}\n",
            "{'loss': 3.1594, 'grad_norm': 0.7522596120834351, 'learning_rate': 2.5316455696202535e-06, 'epoch': 0.88}\n",
            "{'loss': 3.4042, 'grad_norm': 1.1143990755081177, 'learning_rate': 1.6877637130801689e-06, 'epoch': 0.92}\n",
            "{'loss': 2.9799, 'grad_norm': 0.6875190734863281, 'learning_rate': 8.438818565400844e-07, 'epoch': 0.96}\n",
            "{'loss': 3.1811, 'grad_norm': 0.863736093044281, 'learning_rate': 0.0, 'epoch': 1.0}\n",
            "100% 250/250 [33:23<00:00,  6.80s/it]\n",
            "  0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            " 67% 2/3 [00:02<00:01,  1.21s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 4.295854568481445, 'eval_runtime': 6.1196, 'eval_samples_per_second': 1.634, 'eval_steps_per_second': 0.49, 'epoch': 1.0}\n",
            "100% 250/250 [33:29<00:00,  6.80s/it]\n",
            "100% 3/3 [00:05<00:00,  1.61s/it]\u001b[A\n",
            "{'train_runtime': 2010.7881, 'train_samples_per_second': 0.496, 'train_steps_per_second': 0.124, 'train_loss': 3.136001777648926, 'epoch': 1.0}\n",
            "100% 250/250 [33:30<00:00,  8.04s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =  3760509GF\n",
            "  train_loss               =      3.136\n",
            "  train_runtime            = 0:33:30.78\n",
            "  train_samples            =       1000\n",
            "  train_samples_per_second =      0.496\n",
            "  train_steps_per_second   =      0.124\n",
            "\u001b[32m2025-03-18 17:36:49.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m788\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 2010.7881, 'train_samples_per_second': 0.496, 'train_steps_per_second': 0.124, 'total_flos': 4037816683468800.0, 'train_loss': 3.136001777648926, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
            "\u001b[32m2025-03-18 17:36:49.643\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m789\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
            "\u001b[32m2025-03-18 17:36:50.511\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m798\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 3/3 [00:05<00:00,  1.74s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_loss               =     4.2959\n",
            "  eval_runtime            = 0:00:06.09\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      1.639\n",
            "  eval_steps_per_second   =      0.492\n",
            "  perplexity              =    73.3949\n",
            "\u001b[32m2025-03-18 17:36:56.616\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m811\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 4.295854568481445, 'eval_runtime': 6.0996, 'eval_samples_per_second': 1.639, 'eval_steps_per_second': 0.492, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 73.39490862858457}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python supervised_finetuning.py \\\n",
        "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --use_peft True \\\n",
        "    --bf16 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.05 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --evaluation_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --preprocessing_num_workers 1 \\\n",
        "    --output_dir outputs-sft-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tJaxba8rAIb5",
        "outputId": "7c7d51e7-87d4-49a5-d3d0-a580122b70de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 47M\n",
            "-rw-r--r-- 1 root root  816 Mar 18 17:36 adapter_config.json\n",
            "-rw-r--r-- 1 root root  36M Mar 18 17:36 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  429 Mar 18 17:36 all_results.json\n",
            "drwxr-xr-x 2 root root 4.0K Mar 18 17:36 \u001b[0m\u001b[01;34mcheckpoint-250\u001b[0m/\n",
            "-rw-r--r-- 1 root root  219 Mar 18 17:36 eval_results.json\n",
            "-rw-r--r-- 1 root root 5.0K Mar 18 17:36 README.md\n",
            "drwxr-xr-x 3 root root 4.0K Mar 18 17:03 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  485 Mar 18 17:36 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 6.7K Mar 18 17:36 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Mar 18 17:36 tokenizer.json\n",
            "-rw-r--r-- 1 root root 6.0K Mar 18 17:36 trainer_state.json\n",
            "-rw-r--r-- 1 root root  230 Mar 18 17:36 train_results.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-sft-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UkFRtFLDAIb5"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5dQgkBu4AIb5"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "7_HCeoqlglUb",
        "outputId": "a0a4f5b9-006c-497e-fa3b-cd1245716a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Ananlia` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Ananlia`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z-DKleNSAIb5",
        "outputId": "e45f9832-6d29-4ad9-8052-42d8800cc1f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-18 18:31:15.436851: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742322675.464861   37002 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742322675.473490   37002 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-18 18:31:15.507627: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "LoRA model: outputs-sft-v1\n",
            "Loading LoRA for causal language model\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n",
            "Done! model saved to deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft/\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --lora_model outputs-sft-v1 --output_dir deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N2dpHHLiAIb6",
        "outputId": "9cc6e211-02a3-4fbc-a7bf-2047013eba90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3.4G\n",
            "-rw-r--r-- 1 root root  767 Mar 18 18:31 config.json\n",
            "-rw-r--r-- 1 root root  181 Mar 18 18:31 generation_config.json\n",
            "-rw-r--r-- 1 root root 3.4G Mar 18 18:32 model.safetensors\n",
            "-rw-r--r-- 1 root root  485 Mar 18 18:31 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 6.7K Mar 18 18:31 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Mar 18 18:31 tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OEoyRZGrAIb6",
        "outputId": "ba0b8c5d-958c-4d24-b31d-f612860c81aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"_name_or_path\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151643,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.49.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_mrope\": false,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "%cat deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fPQGEaWyAIb6"
      },
      "source": [
        "Stage2 SFT训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:07:40.752635Z",
          "start_time": "2023-06-15T14:07:40.731186Z"
        },
        "id": "i0cZvYIgAIb6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jYG0irUFAIb6"
      },
      "source": [
        "# Stage 3: Reward Modeling\n",
        "\n",
        "第三阶段：RM(Reward Model)奖励模型建模，构造人类偏好排序数据集，训练奖励模型，用来对齐人类偏好，主要是\"HHH\"原则，具体是\"helpful, honest, harmless\"\n",
        "\n",
        "| Stage 3: Reward Modeling        |  [reward_modeling.py](https://github.com/shibing624/MedicalGPT/blob/main/reward_modeling.py) | [run_rm.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rm.sh)    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "G5UJsUAEAIb7"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage2得到的SFT模型\n",
        "2. 数据集：RM阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "MWD-cEtWAIb7"
      },
      "source": [
        "## Stage3 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载模型和tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B32MUuDyAIb7",
        "outputId": "da5b3528-1453-4e82-f2dd-450cb4d7e6f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dpo_zh_500.jsonl\n"
          ]
        }
      ],
      "source": [
        "%ls ./data/reward/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "collapsed": true,
        "id": "byDbV_YiAIb7",
        "outputId": "09b174ed-872c-44b1-9218-ecea55dc90e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-18 18:46:59.548155: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742323619.571089   41207 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742323619.577934   41207 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-18 18:46:59.601777: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "\u001b[32m2025-03-18 18:47:03.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m332\u001b[0m - \u001b[1mModel args: ModelArguments(model_name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft', tokenizer_name_or_path=None, load_in_4bit=False, load_in_8bit=False, cache_dir=None, use_fast_tokenizer=False, torch_dtype='float32', device_map='auto', trust_remote_code=True)\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:03.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m333\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/reward', validation_file_dir='./data/reward', max_source_length=256, max_target_length=256, max_train_samples=1000, max_eval_samples=10, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4)\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:03.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m334\u001b[0m - \u001b[1mTraining args: TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=30000,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=50,\n",
            "eval_strategy=IntervalStrategy.STEPS,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=True,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=outputs-rm-v1/runs/Mar18_18-47-03_5e53693f8c97,\n",
            "logging_first_step=True,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=1.0,\n",
            "optim=OptimizerNames.ADAMW_TORCH,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=outputs-rm-v1,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=False,\n",
            "report_to=['tensorboard'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=outputs-rm-v1,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=SaveStrategy.STEPS,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.05,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.001,\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:03.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m335\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, template_name='vicuna')\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:03.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m336\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\u001b[0m\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\u001b[32m2025-03-18 18:47:09.773\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m398\u001b[0m - \u001b[34m\u001b[1mTokenizer: LlamaTokenizerFast(name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft', vocab_size=151643, model_max_length=16384, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
            "\t151643: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151644: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151645: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151646: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151647: AddedToken(\"<|EOT|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151648: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151649: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
            "}\n",
            ")\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:09.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m401\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:09.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m406\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:09.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m415\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:09.774\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m416\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
            "trainable params: 9,233,920 || all params: 1,552,949,760 || trainable%: 0.5946\n",
            "\u001b[32m2025-03-18 18:47:10.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m459\u001b[0m - \u001b[1mtrain files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:10.071\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m464\u001b[0m - \u001b[1meval files: ./data/reward/dpo_zh_500.jsonl\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:10.783\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m485\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:10.786\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m533\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:10.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m547\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 339\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:10.919\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m548\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:10.921\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m549\u001b[0m - \u001b[34m\u001b[1m<｜begin▁of▁sentence｜>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 我希望你能扮演一个专家的角色。你对于旅行规划的所有信息了如指掌。我会就旅行规划中的不同主题向你提问，你需要给我清晰、简洁和准确的信息。请确保你回答问题时充满自信。 \n",
            "\n",
            "主题 = 旅行规划 ASSISTANT:当然！我在这里可以帮助您解答任何关于旅行规划的问题。请随意问我任何与这个话题相关的问题，我会为您提供清晰、简洁和准确的信息。我会以礼貌、乐于助人和尊重的方式来帮助您，同时确保我的回答不包含任何有害或不道德的内容。\n",
            "您有关于旅行规划的具体问题吗？也许您正在寻找去哪里、如何规划行程或到达目的地后该做什么的建议？无论您有什么问题，请不要犹豫，我会尽力帮助您。\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:10.922\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m562\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'system': '', 'history': [], 'question': '20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅', 'response_chosen': '这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\\n\\n1. “品尝Dishes新鲜果汁，感受不同！”\\n2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\\n3. “用一杯清新的Dishes果汁开启您的一天！”\\n4. “每一口Dishes新鲜果汁都是大自然的味道！”\\n5. “Dishes：新鲜果汁是焦点！”\\n6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\\n7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\\n8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\\n9. “解渴滋养心灵，品尝Dishes美味果汁！”\\n10. “Dishes：每一口都是完美的味道！”\\n11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\\n12. “从农场到餐桌，Dishes果汁充满天然好处！”\\n13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\\n14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\\n15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\\n16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\\n17. “用Dishes招牌果汁混合物提升您的用餐体验！”\\n18. “健康饮品的清新转变 - Dishes果汁必尝！”\\n19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\\n20. “Dishes：果汁永远新鲜，味道永远美味！”', 'response_rejected': '1. \"与菜肴一起品尝新鲜！\"\\n2. \"菜肴：新鲜果汁，新的开始！\"\\n3. \"用菜肴的新鲜混合果汁提神！\"\\n4. \"菜肴，新鲜就是最好的\"\\n5. \"在菜肴庆祝新鲜\"\\n6. \"与菜肴的新鲜果汁为健康干杯\"\\n7. \"在菜肴发现新鲜的魔力\"\\n8. \"品尝菜肴的新鲜果汁，感受不同\"\\n9. \"在菜肴解锁新鲜\"\\n10. \"用菜肴的新鲜果汁迎接新的一天\"\\n11. \"在菜肴，每天都有新鲜\"\\n12. \"用菜肴的新鲜果汁获得能量\"\\n13. \"在菜肴为生活喝果汁\"\\n14. \"拥抱健康，享受菜肴的新鲜果汁\"\\n15. \"菜肴：新鲜与美味的交汇处\"\\n16. \"在菜肴体验新鲜的力量\"\\n17. \"菜肴：把健康送到你家门口\"\\n18. \"像微风一样清新，菜肴的果汁\"\\n19. \"生命太短暂，只为菜肴的新鲜果汁\"\\n20. \"菜肴：新鲜始终是你一天的首选\"'}\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:11.050\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m575\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 5\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:11.050\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m576\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
            "\u001b[32m2025-03-18 18:47:11.054\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m577\u001b[0m - \u001b[34m\u001b[1m<｜begin▁of▁sentence｜>A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 20个关于新鲜果汁菜单的口号，适用于一家名为\"Dishes\"的餐厅 ASSISTANT:这里是一个名为“Dishes”的餐厅的20个口号，突出了其新鲜果汁菜单：\n",
            "\n",
            "1. “品尝Dishes新鲜果汁，感受不同！”\n",
            "2. “新鲜榨取，直达您的餐桌 - Dishes果汁纯享！”\n",
            "3. “用一杯清新的Dishes果汁开启您的一天！”\n",
            "4. “每一口Dishes新鲜果汁都是大自然的味道！”\n",
            "5. “Dishes：新鲜果汁是焦点！”\n",
            "6. “满足您的口腹之欲，享用Dishes口水直流的农场果汁！”\n",
            "7. “新鲜果汁，新鲜味道，新鲜菜肴 - 这是Dishes的承诺！”\n",
            "8. “用Dishes营养果汁获得每日所需的维生素和矿物质！”\n",
            "9. “解渴滋养心灵，品尝Dishes美味果汁！”\n",
            "10. “Dishes：每一口都是完美的味道！”\n",
            "11. “新鲜制作，完美平衡 - Dishes果汁是感官的享受！”\n",
            "12. “从农场到餐桌，Dishes果汁充满天然好处！”\n",
            "13. “踏入Dishes，品尝我们新鲜果汁的甜蜜！”\n",
            "14. “用Dishes 100%新鲜水果果汁呵护您的身体！”\n",
            "15. “Dishes：每一杯果汁都是用激情和关怀精心制作！”\n",
            "16. “沉醉于Dishes新鲜榨取果汁的健康热情！”\n",
            "17. “用Dishes招牌果汁混合物提升您的用餐体验！”\n",
            "18. “健康饮品的清新转变 - Dishes果汁必尝！”\n",
            "19. “加入Dishes的新鲜果汁革命 - 您的味蕾会感激您！”\n",
            "20. “Dishes：果汁永远新鲜，味道永远美味！”\u001b[0m\n",
            "/content/MedicalGPT/reward_modeling.py:590: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `RewardTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = RewardTrainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[32m2025-03-18 18:47:11.102\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m604\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "\u001b[32m2025-03-18 18:47:11.138\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m605\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids_chosen': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151646,  56568, 101909,  15469, 106575,   1773,  99553, 109648, 102349,\n",
            "           3837,  32555,  20002, 104689,  18493, 105413,  78973, 104077, 101128,\n",
            "         102349,  37132,     82,     29,   6448,     25,  38903,    228, 100697,\n",
            "         101038,  46944, 101339, 109784,   3837,  99517,  99461,  80443,  99428,\n",
            "          34187,   3837,  99360,  99927,  99945, 104132, 102182,  69249, 108804,\n",
            "         105950,   1773,  62244,  56007,   2073, 100584, 100697, 103922,  36993,\n",
            "         104139, 100681,  11319,  33590,   2073, 105750,    854, 101909, 104775,\n",
            "         102349, 101037,  94432, 102349,  20412,   5122,  35560,   3846,   2821,\n",
            "             25, 101068,  60610,   2183,   6130, 103922,  36993, 104139, 100535,\n",
            "         101224,   3837,  99519,  99605, 108876,  33108, 101128, 104309, 115742,\n",
            "           1773, 103968,  96050, 100137, 104705,  41505, 105750,    854,  87267,\n",
            "         102095,  32664,   2183,   6130, 101224,  31235, 102188,   9370,  53481,\n",
            "           1773, 106124,   3837,   2183,   6130, 104309,  99519, 100364, 101106,\n",
            "         104028, 104056,  87140,  99329, 101904,  68536, 104048, 112321,   5373,\n",
            "         118009,  57191,  18830, 112321,  63109,   1773, 105750, 108063, 102119,\n",
            "         109228,  32664,  99569,  17340, 109955,  99539, 100271,  33108, 100765,\n",
            "          96050, 100137, 104705,  87267, 104605, 101073,   3837, 104033,  62244,\n",
            "           2183,   6130, 100684, 100690, 100720,  99487, 101339,   1773]],\n",
            "       device='cuda:0'), 'attention_mask_chosen': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'input_ids_rejected': tensor([[151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151646,\n",
            "          56568, 101909,  15469, 106575,   1773,  99553, 109648, 102349,   3837,\n",
            "          32555,  20002, 104689,  18493, 105413,  78973, 104077, 101128, 102349,\n",
            "          37132,     82,     29,   6448,     25,  38903,    228, 100697, 101038,\n",
            "          46944, 101339, 109784,   3837,  99517,  99461,  80443,  99428,  34187,\n",
            "           3837,  99360,  99927,  99945, 104132, 102182,  69249, 108804, 105950,\n",
            "           1773,  62244,  56007,   2073, 100584, 100697, 103922,  36993, 104139,\n",
            "         100681,  11319,  33590,   2073, 105750,    854, 101909, 104775, 102349,\n",
            "         101037,  94432, 102349,  20412,   5122,  35560,   3846,   2821,     25,\n",
            "         100345, 103008,  27369,   3837,   2183,   6130, 101038, 101339,  99366,\n",
            "          87140,  99329,  62926,  99360,  99927,  99945, 114871, 102182, 100622,\n",
            "         105950,  33447,   3837, 102342,  87267, 100394, 105750, 104336,   1773,\n",
            "          99917, 104506,  48443,     16,     13,  84238,    118, 100467, 102193,\n",
            "          27369,   5122, 108024,  15946, 105283, 110329, 102406,   2183,   6130,\n",
            "          33108, 101339, 110117, 109977, 104612,  57191,  99605,  72064,   1773,\n",
            "          80443, 100656, 104754,  57191, 100656, 110257,   3837,   2183,   6130,\n",
            "         102342,  87267,  44063, 101339, 104796, 105257,  17714, 105750,   8997,\n",
            "             17,     13,  90476,    100,  62922, 108140,   5122, 102630,  99519,\n",
            "          46944, 102015, 101038,  46944, 101989,  99366,  87140,  99329,  62926,\n",
            "         100669,  99927,  99945, 100622, 105950,  68536, 100394, 105750, 102222,\n",
            "         105424, 101158, 109391, 108140,   3837, 103980,  34187, 101063, 105106,\n",
            "          33108, 101968,   9370, 108589,  99483,  87531, 101507,   8997,     18,\n",
            "             13,  84238,    118, 100467, 117072,   5122, 101339,  18493,  57218,\n",
            "           2183,   6130,   9370, 104199,  15946,  80443, 107837,  99885, 117072,\n",
            "          57191, 108465,  33071,   1773,  99517, 100009,  18493,  99366,  87140,\n",
            "          99329,  62926, 100669,  99927,  99945, 100622, 105950,   3837,  43288,\n",
            "         100684, 102406,  99517,  18830, 105750, 110257,  57191, 111450,   8997,\n",
            "             19,     13,  86009, 112449,   9370, 102193,   5122,  99329,  99928,\n",
            "          20412, 100659,  85336, 109784,  33108,  57218,  99614, 102470,   9370,\n",
            "         117262,   1773, 108019, 105750, 104199,   9370, 102618, 102325,   3837,\n",
            "           2183,   6130, 102342,  87267, 108939, 104705,  44063, 101339, 104796,\n",
            "         105257,  17714, 105750,   3407, 101886,  41505, 105750,    854,  99520,\n",
            "           2183,   6130, 103922,  36993,  99996, 101224, 107474, 102349,   1773,\n",
            "          46944,  33126, 106873, 102349, 104560,   2073, 102962,    854,  57191,\n",
            "           2073, 103198,  33590,  99519,   2183,   6130,  87267,  32664,  99794,\n",
            "         101339, 105628, 101139,  33108, 100565, 103198,   1773, 101948,   3837,\n",
            "           2183,   6130,  87267, 100009, 109136, 101339, 100669, 100648,  99927,\n",
            "          99945, 100622, 105950,   9370, 118009,  33108, 115457,   1773]],\n",
            "       device='cuda:0'), 'attention_mask_rejected': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'return_loss': True}\u001b[0m\n",
            "  0% 0/339 [00:00<?, ?it/s]Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
            "{'loss': 0.7524, 'grad_norm': 19.046031951904297, 'learning_rate': 1.1764705882352942e-06, 'epoch': 0.0}\n",
            "{'loss': 0.6209, 'grad_norm': 16.18047523498535, 'learning_rate': 1.1764705882352942e-05, 'epoch': 0.03}\n",
            "{'loss': 0.7097, 'grad_norm': 25.11884880065918, 'learning_rate': 1.9813664596273293e-05, 'epoch': 0.06}\n",
            "{'loss': 0.9754, 'grad_norm': 13.045881271362305, 'learning_rate': 1.9192546583850932e-05, 'epoch': 0.09}\n",
            "{'loss': 1.0439, 'grad_norm': 6.970424652099609, 'learning_rate': 1.8571428571428575e-05, 'epoch': 0.12}\n",
            "{'loss': 0.7153, 'grad_norm': 20.315690994262695, 'learning_rate': 1.795031055900621e-05, 'epoch': 0.15}\n",
            " 15% 50/339 [02:14<13:23,  2.78s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 2/5 [00:00<00:01,  2.30it/s]\u001b[A\n",
            " 60% 3/5 [00:01<00:01,  1.62it/s]\u001b[A\n",
            " 80% 4/5 [00:02<00:00,  1.41it/s]\u001b[A\n",
            "                                    \n",
            "\u001b[A{'eval_loss': 0.8206707239151001, 'eval_mse': 0.3889577388763428, 'eval_mae': 0.5411028861999512, 'eval_runtime': 4.3694, 'eval_samples_per_second': 1.144, 'eval_steps_per_second': 1.144, 'epoch': 0.15}\n",
            " 15% 50/339 [02:18<13:23,  2.78s/it]\n",
            "100% 5/5 [00:03<00:00,  1.30it/s]\u001b[A\n",
            "{'loss': 0.7295, 'grad_norm': 35.290733337402344, 'learning_rate': 1.7329192546583854e-05, 'epoch': 0.18}\n",
            "{'loss': 0.3361, 'grad_norm': 2.3176121711730957, 'learning_rate': 1.670807453416149e-05, 'epoch': 0.21}\n",
            "{'loss': 0.6689, 'grad_norm': 12.579358100891113, 'learning_rate': 1.6086956521739132e-05, 'epoch': 0.24}\n",
            "{'loss': 0.8034, 'grad_norm': 23.442462921142578, 'learning_rate': 1.5465838509316772e-05, 'epoch': 0.27}\n",
            "{'loss': 1.1783, 'grad_norm': 38.93765640258789, 'learning_rate': 1.4844720496894411e-05, 'epoch': 0.29}\n",
            " 29% 100/339 [04:35<10:56,  2.75s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 2/5 [00:00<00:01,  2.28it/s]\u001b[A\n",
            " 60% 3/5 [00:01<00:01,  1.59it/s]\u001b[A\n",
            " 80% 4/5 [00:02<00:00,  1.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.804299533367157, 'eval_mse': 0.35735973715782166, 'eval_mae': 0.5227199792861938, 'eval_runtime': 4.4431, 'eval_samples_per_second': 1.125, 'eval_steps_per_second': 1.125, 'epoch': 0.29}\n",
            " 29% 100/339 [04:40<10:56,  2.75s/it]\n",
            "100% 5/5 [00:03<00:00,  1.28it/s]\u001b[A\n",
            "{'loss': 1.1247, 'grad_norm': 13.501296043395996, 'learning_rate': 1.422360248447205e-05, 'epoch': 0.32}\n",
            "{'loss': 0.7395, 'grad_norm': 30.86153793334961, 'learning_rate': 1.3602484472049691e-05, 'epoch': 0.35}\n",
            "{'loss': 0.7755, 'grad_norm': 28.93402099609375, 'learning_rate': 1.2981366459627329e-05, 'epoch': 0.38}\n",
            "{'loss': 0.8861, 'grad_norm': 36.09565734863281, 'learning_rate': 1.236024844720497e-05, 'epoch': 0.41}\n",
            "{'loss': 0.7548, 'grad_norm': 20.393678665161133, 'learning_rate': 1.1739130434782611e-05, 'epoch': 0.44}\n",
            " 44% 150/339 [06:58<08:45,  2.78s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 2/5 [00:00<00:01,  2.26it/s]\u001b[A\n",
            " 60% 3/5 [00:01<00:01,  1.59it/s]\u001b[A\n",
            " 80% 4/5 [00:02<00:00,  1.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7589091062545776, 'eval_mse': 0.3779890835285187, 'eval_mae': 0.5237917304039001, 'eval_runtime': 4.4482, 'eval_samples_per_second': 1.124, 'eval_steps_per_second': 1.124, 'epoch': 0.44}\n",
            " 44% 150/339 [07:03<08:45,  2.78s/it]\n",
            "100% 5/5 [00:03<00:00,  1.28it/s]\u001b[A\n",
            "{'loss': 0.7981, 'grad_norm': 23.64375114440918, 'learning_rate': 1.1118012422360249e-05, 'epoch': 0.47}\n",
            "{'loss': 0.9193, 'grad_norm': 34.03143310546875, 'learning_rate': 1.049689440993789e-05, 'epoch': 0.5}\n",
            "{'loss': 0.7271, 'grad_norm': 21.434494018554688, 'learning_rate': 9.875776397515529e-06, 'epoch': 0.53}\n",
            "{'loss': 0.8171, 'grad_norm': 8.134841918945312, 'learning_rate': 9.254658385093168e-06, 'epoch': 0.56}\n",
            "{'loss': 0.972, 'grad_norm': 36.78045654296875, 'learning_rate': 8.633540372670808e-06, 'epoch': 0.59}\n",
            " 59% 200/339 [09:21<06:20,  2.74s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 2/5 [00:00<00:01,  2.29it/s]\u001b[A\n",
            " 60% 3/5 [00:01<00:01,  1.60it/s]\u001b[A\n",
            " 80% 4/5 [00:02<00:00,  1.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7606998682022095, 'eval_mse': 0.41436854004859924, 'eval_mae': 0.5471183061599731, 'eval_runtime': 4.4215, 'eval_samples_per_second': 1.131, 'eval_steps_per_second': 1.131, 'epoch': 0.59}\n",
            " 59% 200/339 [09:25<06:20,  2.74s/it]\n",
            "100% 5/5 [00:03<00:00,  1.28it/s]\u001b[A\n",
            "{'loss': 0.5976, 'grad_norm': 24.32565689086914, 'learning_rate': 8.012422360248447e-06, 'epoch': 0.62}\n",
            "{'loss': 0.7219, 'grad_norm': 9.510354995727539, 'learning_rate': 7.391304347826087e-06, 'epoch': 0.65}\n",
            "{'loss': 0.6856, 'grad_norm': 21.54477310180664, 'learning_rate': 6.7701863354037265e-06, 'epoch': 0.68}\n",
            "{'loss': 0.7408, 'grad_norm': 10.090606689453125, 'learning_rate': 6.1490683229813675e-06, 'epoch': 0.71}\n",
            "{'loss': 0.7007, 'grad_norm': 35.476444244384766, 'learning_rate': 5.527950310559007e-06, 'epoch': 0.74}\n",
            " 74% 250/339 [11:43<04:07,  2.79s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 2/5 [00:00<00:01,  2.25it/s]\u001b[A\n",
            " 60% 3/5 [00:01<00:01,  1.59it/s]\u001b[A\n",
            " 80% 4/5 [00:02<00:00,  1.38it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7528471946716309, 'eval_mse': 0.455922931432724, 'eval_mae': 0.5711184740066528, 'eval_runtime': 4.454, 'eval_samples_per_second': 1.123, 'eval_steps_per_second': 1.123, 'epoch': 0.74}\n",
            " 74% 250/339 [11:48<04:07,  2.79s/it]\n",
            "100% 5/5 [00:03<00:00,  1.28it/s]\u001b[A\n",
            "{'loss': 0.6342, 'grad_norm': 9.446256637573242, 'learning_rate': 4.906832298136646e-06, 'epoch': 0.77}\n",
            "{'loss': 0.7204, 'grad_norm': 8.403082847595215, 'learning_rate': 4.2857142857142855e-06, 'epoch': 0.8}\n",
            "{'loss': 0.6956, 'grad_norm': 29.51999282836914, 'learning_rate': 3.664596273291926e-06, 'epoch': 0.83}\n",
            "{'loss': 0.8739, 'grad_norm': 60.22491455078125, 'learning_rate': 3.043478260869566e-06, 'epoch': 0.86}\n",
            "{'loss': 0.8599, 'grad_norm': 33.7528076171875, 'learning_rate': 2.422360248447205e-06, 'epoch': 0.88}\n",
            " 88% 300/339 [14:06<01:46,  2.73s/it]\n",
            "  0% 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 40% 2/5 [00:00<00:01,  2.27it/s]\u001b[A\n",
            " 60% 3/5 [00:01<00:01,  1.60it/s]\u001b[A\n",
            " 80% 4/5 [00:02<00:00,  1.39it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.7528173327445984, 'eval_mse': 0.43455392122268677, 'eval_mae': 0.5626698732376099, 'eval_runtime': 4.4209, 'eval_samples_per_second': 1.131, 'eval_steps_per_second': 1.131, 'epoch': 0.88}\n",
            " 88% 300/339 [14:10<01:46,  2.73s/it]\n",
            "100% 5/5 [00:03<00:00,  1.28it/s]\u001b[A\n",
            "{'loss': 0.8442, 'grad_norm': 72.02084350585938, 'learning_rate': 1.8012422360248449e-06, 'epoch': 0.91}\n",
            "{'loss': 1.0269, 'grad_norm': 12.643758773803711, 'learning_rate': 1.1801242236024846e-06, 'epoch': 0.94}\n",
            "{'loss': 0.4888, 'grad_norm': 48.513092041015625, 'learning_rate': 5.590062111801243e-07, 'epoch': 0.97}\n",
            "{'train_runtime': 958.9363, 'train_samples_per_second': 0.354, 'train_steps_per_second': 0.354, 'train_loss': 0.7855230546982239, 'epoch': 1.0}\n",
            "100% 339/339 [15:58<00:00,  2.83s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =        1.0\n",
            "  total_flos               =        0GF\n",
            "  train_loss               =     0.7855\n",
            "  train_runtime            = 0:15:58.93\n",
            "  train_samples            =        500\n",
            "  train_samples_per_second =      0.354\n",
            "  train_steps_per_second   =      0.354\n",
            "\u001b[32m2025-03-18 19:03:10.548\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m619\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 958.9363, 'train_samples_per_second': 0.354, 'train_steps_per_second': 0.354, 'total_flos': 0.0, 'train_loss': 0.7855230546982239, 'epoch': 1.0, 'train_samples': 500}\u001b[0m\n",
            "\u001b[32m2025-03-18 19:03:10.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m620\u001b[0m - \u001b[1mSaving model checkpoint to outputs-rm-v1\u001b[0m\n",
            "\u001b[32m2025-03-18 19:03:10.884\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m625\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
            "100% 5/5 [00:03<00:00,  1.41it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        1.0\n",
            "  eval_loss               =      0.751\n",
            "  eval_mae                =     0.5628\n",
            "  eval_mse                =     0.4372\n",
            "  eval_runtime            = 0:00:04.43\n",
            "  eval_samples            =         10\n",
            "  eval_samples_per_second =      1.127\n",
            "  eval_steps_per_second   =      1.127\n",
            "  perplexity              =     2.1192\n",
            "\u001b[32m2025-03-18 19:03:15.323\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m637\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 0.7510163187980652, 'eval_mse': 0.43719974160194397, 'eval_mae': 0.5628412961959839, 'eval_runtime': 4.4351, 'eval_samples_per_second': 1.127, 'eval_steps_per_second': 1.127, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 2.119152657224333}\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python reward_modeling.py \\\n",
        "    --model_name_or_path deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft \\\n",
        "    --train_file_dir ./data/reward \\\n",
        "    --validation_file_dir ./data/reward \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --do_train \\\n",
        "    --use_peft True \\\n",
        "    --seed 42 \\\n",
        "    --max_train_samples 1000 \\\n",
        "    --max_eval_samples 10 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --weight_decay 0.001 \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 10 \\\n",
        "    --eval_steps 50 \\\n",
        "    --evaluation_strategy steps \\\n",
        "    --save_steps 500 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_total_limit 3 \\\n",
        "    --max_source_length 256 \\\n",
        "    --max_target_length 256 \\\n",
        "    --output_dir outputs-rm-v1 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --ddp_timeout 30000 \\\n",
        "    --logging_first_step True \\\n",
        "    --target_modules all \\\n",
        "    --lora_rank 8 \\\n",
        "    --lora_alpha 16 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --torch_dtype float32 \\\n",
        "    --device_map auto \\\n",
        "    --report_to tensorboard \\\n",
        "    --ddp_find_unused_parameters False \\\n",
        "    --remove_unused_columns False \\\n",
        "    --gradient_checkpointing True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "y-hXHA8jAIb7",
        "outputId": "b829cd3a-30ba-4219-d9b7-edde01ed93d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 47M\n",
            "-rw-r--r-- 1 root root  849 Mar 18 19:03 adapter_config.json\n",
            "-rw-r--r-- 1 root root  36M Mar 18 19:03 adapter_model.safetensors\n",
            "-rw-r--r-- 1 root root  487 Mar 18 19:03 all_results.json\n",
            "drwxr-xr-x 2 root root 4.0K Mar 18 19:03 \u001b[0m\u001b[01;34mcheckpoint-339\u001b[0m/\n",
            "-rw-r--r-- 1 root root  293 Mar 18 19:03 eval_results.json\n",
            "-rw-r--r-- 1 root root 5.0K Mar 18 19:03 README.md\n",
            "drwxr-xr-x 6 root root 4.0K Mar 18 18:47 \u001b[01;34mruns\u001b[0m/\n",
            "-rw-r--r-- 1 root root  485 Mar 18 19:03 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root 6.7K Mar 18 19:03 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root  11M Mar 18 19:03 tokenizer.json\n",
            "-rw-r--r-- 1 root root 8.4K Mar 18 19:03 trainer_state.json\n",
            "-rw-r--r-- 1 root root  214 Mar 18 19:03 train_results.json\n"
          ]
        }
      ],
      "source": [
        "%ls -lh outputs-rm-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QexN74gMAIb8"
      },
      "source": [
        "模型训练结果：\n",
        "- 使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DRz7jb_AAIb8"
      },
      "source": [
        "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "XZ5Wqlgtp--p",
        "outputId": "eccd0a9e-107f-4d48-83a3-a48847dc9322",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `Ananlia` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `Ananlia`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1DeuEs5AIb8",
        "outputId": "04529cf5-f330-4be5-b772-dc52080b1a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-03-18 19:11:00.104173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1742325060.134234   47147 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1742325060.143641   47147 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-03-18 19:11:00.183873: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Namespace(base_model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft', tokenizer_path=None, lora_model='outputs-rm-v1', resize_emb=False, output_dir='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft-rm/', hf_hub_model_id='', hf_hub_token=None)\n",
            "Base model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft\n",
            "LoRA model: outputs-rm-v1\n",
            "Loading LoRA for sequence classification model\n",
            "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Merging with merge_and_unload...\n",
            "Saving to Hugging Face format...\n"
          ]
        }
      ],
      "source": [
        "!python merge_peft_adapter.py \\\n",
        "    --base_model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft --lora_model outputs-rm-v1 --output_dir deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B-sft-rm/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkijAtI7AIb9"
      },
      "outputs": [],
      "source": [
        "%ls -lh merged-rm/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYq2PXF8AIcB"
      },
      "outputs": [],
      "source": [
        "%cat merged-rm/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "n74EeWErAIcD"
      },
      "source": [
        "Stage3 奖励建模第一次训练完成。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-15T14:12:09.472414Z",
          "start_time": "2023-06-15T14:12:09.464881Z"
        },
        "id": "O_wsCHWFAIcD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4o2_hzgDAIcD"
      },
      "source": [
        "# Stage 4: Reinforcement Learning Training\n",
        "\n",
        "第四阶段：RL(Reinforcement Learning)基于人类反馈的强化学习(RLHF)，用奖励模型来训练SFT模型，生成模型使用奖励或惩罚来更新其策略，以便生成更高质量、更符合人类偏好的文本\n",
        "\n",
        "| Stage 4: Reinforcement Learning |  [rl_training.py](https://github.com/shibing624/MedicalGPT/blob/main/rl_training.py) | [run_rl.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_rl.sh)    |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "vMMktDV-AIcE"
      },
      "source": [
        "#### 说明：\n",
        "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型、奖励模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
        "\n",
        "1. 生成模型：使用的是Qwen/Qwen2.5-0.5B 或者 Stage2得到的SFT模型\n",
        "2. 奖励模型：使用的是`OpenAssistant/reward-model-deberta-v3-large-v2` 或者 Stage3得到的BERT类或者GPT类奖励模型\n",
        "3. 数据集：RL阶段的数据可以复用SFT的数据集，使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "13ZU9QxwAIcE"
      },
      "source": [
        "## Stage4 咱们开始吧\n",
        "\n",
        "训练步骤如下：\n",
        "\n",
        "1. 确认训练集\n",
        "2. 执行训练脚本\n",
        "\n",
        "训练脚本的执行逻辑如下：\n",
        "1. 导入依赖包\n",
        "2. 设置参数\n",
        "3. 定义各函数并加载训练集\n",
        "4. 加载生成模型和tokenizer，加载奖励模型和其tokenizer\n",
        "5. 开始训练并评估\n",
        "6. 查看训练结果\n",
        "\n",
        "以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvvVCcB3AIcF"
      },
      "outputs": [],
      "source": [
        "%ls ./data/finetune/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-0WvYkuAIcF"
      },
      "outputs": [],
      "source": [
        "! CUDA_VISIBLE_DEVICES=0 python ppo_training.py \\\n",
        "    --sft_model_path ./merged-sft \\\n",
        "    --reward_model_path ./merged-rm \\\n",
        "    --template_name qwen \\\n",
        "    --torch_dtype bfloat16 \\\n",
        "    --device_map auto \\\n",
        "    --train_file_dir ./data/finetune \\\n",
        "    --validation_file_dir ./data/finetune \\\n",
        "    --max_source_length 256 \\\n",
        "    --response_length 1000 \\\n",
        "    --do_train \\\n",
        "    --save_steps 50 \\\n",
        "    --output_dir outputs-ppo-v1 \\\n",
        "    --num_train_epochs 3 \\\n",
        "    --report_to tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRhKGVnmAIcF"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-ppo-v1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5RS0hbR1AIcF"
      },
      "source": [
        "模型训练结果：\n",
        "- use_peft=False,默认是使用全参训练，模型保存的就是`model-00001-of-00002.safetensors`等文件，配置文件是`config.json`\n",
        "- use_peft=True, 则使用lora训练模型，则保存的lora权重是`adapter_model.safetensors`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
        "- 日志保存在`output_dir/trl`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/trl --host 0.0.0.0 --port 8009`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfM8CwD0AIcG"
      },
      "outputs": [],
      "source": [
        "%ls -lh outputs-ppo-v1/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOgYEcWWAIcG"
      },
      "outputs": [],
      "source": [
        "%cat outputs-ppo-v1/config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "jfDwWaW3AIcG"
      },
      "source": [
        "Stage4 RL第一次训练完成。\n",
        "\n",
        "**至此一个完整的4阶段训练流程演示完成。**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "UcSzGCDjAIcG"
      },
      "source": [
        "实际操作中Stage3和Stage4可以反复多次，直到RL得到的最后模型满足评估要求。\n",
        "\n",
        "RLHF过程可以把SFT模型当成一个初始化模型，RM模型当做指导老师，使用RL(PPO)调教SFT模型生成指导老师最满意的结果，如果小学老师满意了，我们就再训练一个中学老师，继续指导，中学老师满意了，就训练一个大学老师，这样不断迭代，使得生成模型的质量达到甚至超过人工撰写的天花板。\n",
        "\n",
        "RLHF训练不易，此项目提供给大家一种实现的方法和参考，希望抛砖引玉，共同促进中文开源LLM发展。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "xYJYLp-uAIcH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:34:29.658428Z",
          "start_time": "2023-06-26T12:34:29.620609Z"
        },
        "id": "yfBI7V9xAIcH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lkFI4cjXAIcH"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eDGvEazZAIcH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-06-26T12:35:00.864463Z",
          "start_time": "2023-06-26T12:34:47.802087Z"
        },
        "id": "knUJKj_uAIcH"
      },
      "outputs": [],
      "source": [
        "!python inference.py --base_model merged-ppo-v1\n",
        "# 或在shell中运行\n",
        "# !python inference.py --base_model merged-ppo-v1 --interactive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lWqxfnpWAIcI"
      },
      "source": [
        "Input:介绍下南京\n",
        "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
        "\n",
        "完。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8R_Z1hFAIcI"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}